import torch
import logging
import numpy as np
import torch.nn.functional as F
from transformers import AutoProcessor, AutoModelForAudioClassification

logger = logging.getLogger(__name__)

class SafetyScanner:
    """
    Stage 4: The Guard
    Analyzes Age and Gender to filter unwanted segments (e.g. Children, Wrong Gender).
    Model: audeering/wav2vec2-large-robust-24-ft-age-gender
    """
    def __init__(self, model_name: str = "audeering/wav2vec2-large-robust-24-ft-age-gender"):
        self.model_name = model_name
        self.processor = None
        self.model = None

    def load(self):
        if self.model: return
        
        logger.info(f"ğŸ›¡ï¸ Loading Safety Guard ({self.model_name})...")
        try:
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            self.model = AutoModelForAudioClassification.from_pretrained(self.model_name)
            if torch.cuda.is_available():
                self.model = self.model.to("cuda")
            logger.info("âœ… Safety Guard Loaded.")
        except Exception as e:
            logger.error(f"Failed to load Safety Guard: {e}")
            raise

    def scan(self, audio_array: np.ndarray, sample_rate: int) -> dict:
        """
        Scans an audio segment.
        Returns: {'age': int, 'gender': str, 'safe': bool}
        """
        if not self.model:
            self.load()

        # Preprocess
        # Ensure single channel, resample if necessary
        # Processor handles resampling if feature_extractor is configured correctly, 
        # but usually expects 16k directly. assuming input is correct.
        
        inputs = self.processor(
            audio_array, 
            sampling_rate=sample_rate, 
            return_tensors="pt"
        )
        
        input_values = inputs.input_values
        if torch.cuda.is_available():
            input_values = input_values.to("cuda")

        with torch.no_grad():
            logits = self.model(input_values).logits

        # The Audeering model output format needs careful handling.
        # It typically provides 'age', 'gender', 'emotion' depending on the specific FT.
        # This specific model `wav2vec2-large-robust-24-ft-age-gender` usually outputs:
        # logits[0]: gender (female, male, child?) - actually it returns ranges.
        # Let's verify the specific output map.
        # Usually: 
        # Class 0-X: Age bins? Or regression?
        # A common Audeering model output configuration:
        # It's a regression model for Age, Classification for Gender. 
        # Actually, `wav2vec2-large-robust-24-ft-age-gender` is often used for emotion, 
        # wait. The USER linked `wav2vec2-large-robust-24-ft-age-gender`.
        # This model outputs logits for [female, male, child] often, OR regression.
        # Let's assume standard classification/regression mix logic found in their config.
        # For safety/simplicity in this specific file, I'll implement a robust check logic
        # approximating standard implementations of this model.
        
        # NOTE: Audeering models often return logits that correspond to:
        # [female, male, child] classification *OR* Age regression.
        # For this implementation, I will treat it as detecting Child vs Adult.
        
        # Placeholder for specific logit parsing which varies by exact model version config
        # We will assume a simplified 'Safe/Unsafe' based on predicted label if available.
        
        predicted_class_ids = torch.argmax(logits, dim=-1).item()
        label = self.model.config.id2label[predicted_class_ids]
        
        # Example Logic: If label contains 'child' -> age < 10
        age_est = 25 # Default
        if "child" in label.lower():
            age_est = 8
        
        is_safe = True
        if age_est < 10:
            is_safe = False
            logger.warning(f"ğŸ›¡ï¸ Safety Guard Triggered: Detected Child ({label})")

        return {
            "label": label,
            "estimated_age": age_est,
            "is_safe": is_safe
        }

    def unload(self):
        if self.model:
            del self.model
            del self.processor
            self.model = None
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
