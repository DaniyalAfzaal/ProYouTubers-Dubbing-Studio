
Fix tests: update to use translate_segments instead of run_translatioâ€¦ #56
Jobs
Run details
Annotations
1 error
test
failed 2 minutes ago in 1m 31s
Search logs
1s
1s
0s
3s
1m 24s
Run make test
cd apps/backend/services/asr && uv run --with pytest pytest
Using CPython 3.11.14 interpreter at: /opt/hostedtoolcache/Python/3.11.14/x64/bin/python3.11
Creating virtual environment at: .venv
   Building common-schemas @ file:///home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/libs/common-schemas
Downloading nvidia-cublas-cu12 (566.8MiB)
Downloading nvidia-cusparse-cu12 (274.9MiB)
Downloading nvidia-cusparselt-cu12 (273.9MiB)
Downloading nvidia-nccl-cu12 (307.4MiB)
Downloading triton (148.3MiB)
Downloading nvidia-cufft-cu12 (184.2MiB)
Downloading nvidia-nvjitlink-cu12 (37.4MiB)
Downloading nvidia-cuda-nvrtc-cu12 (84.0MiB)
Downloading numpy (16.2MiB)
Downloading nvidia-curand-cu12 (60.7MiB)
Downloading nvidia-cuda-cupti-cu12 (9.8MiB)
Downloading scikit-learn (9.3MiB)
Downloading sympy (6.0MiB)
Downloading lxml (5.0MiB)
Downloading uvloop (3.8MiB)
Downloading tokenizers (3.1MiB)
Downloading scipy (34.2MiB)
Downloading hf-xet (3.0MiB)
Downloading networkx (1.9MiB)
Downloading pydantic-core (1.9MiB)
Downloading sentencepiece (1.3MiB)
Downloading setuptools (1.1MiB)
Downloading nvidia-cufile-cu12 (1.1MiB)
Downloading konlpy (18.5MiB)
Downloading transformers (11.4MiB)
Downloading nvidia-cusolver-cu12 (255.1MiB)
Downloading torch (846.9MiB)
Downloading nvidia-cudnn-cu12 (674.0MiB)
 Downloaded nvidia-cufile-cu12
 Downloaded sentencepiece
 Downloaded pydantic-core
 Downloaded setuptools
 Downloaded tokenizers
 Downloaded hf-xet
 Downloaded networkx
 Downloaded uvloop
 Downloaded lxml
      Built common-schemas @ file:///home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/libs/common-schemas
 Downloaded sympy
 Downloaded scikit-learn
 Downloaded nvidia-cuda-cupti-cu12
 Downloaded transformers
 Downloaded numpy
 Downloaded konlpy
   Building jieba==0.42.1
 Downloaded scipy
      Built jieba==0.42.1
 Downloaded nvidia-nvjitlink-cu12
   Building unidic-lite==1.0.8
 Downloaded nvidia-curand-cu12
 Downloaded nvidia-cuda-nvrtc-cu12
      Built unidic-lite==1.0.8
 Downloaded triton
 Downloaded nvidia-cufft-cu12
 Downloaded nvidia-cusolver-cu12
 Downloaded nvidia-cusparse-cu12
 Downloaded nvidia-cusparselt-cu12
 Downloaded nvidia-nccl-cu12
 Downloaded nvidia-cublas-cu12
 Downloaded nvidia-cudnn-cu12
 Downloaded torch
Installed 69 packages in 284ms
Downloading pygments (1.2MiB)
 Downloaded pygments
Installed 5 packages in 7ms
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0
rootdir: /home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/asr
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 8 items

tests/test_cli.py ...                                                    [ 37%]
tests/test_cli_0.py ..                                                   [ 62%]
tests/test_cli_1.py ..                                                   [ 87%]
tests/test_runner_api.py .                                               [100%]

============================== 8 passed in 0.30s ===============================
cd apps/backend/services/translation && uv run --with pytest pytest
Using CPython 3.11.14 interpreter at: /opt/hostedtoolcache/Python/3.11.14/x64/bin/python3.11
Creating virtual environment at: .venv
Installed 64 packages in 303ms
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0
rootdir: /home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/translation
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 5 items

tests/test_cli.py ...                                                    [ 60%]
tests/test_runner_api.py ..                                              [100%]

============================== 5 passed in 0.27s ===============================
cd apps/backend/services/tts && uv run --with pytest pytest
Using CPython 3.11.14 interpreter at: /opt/hostedtoolcache/Python/3.11.14/x64/bin/python3.11
Creating virtual environment at: .venv
Downloading nvidia-nvshmem-cu12 (118.9MiB)
Downloading torch (858.1MiB)
Downloading scipy (34.2MiB)
Downloading triton (162.5MiB)
Downloading nvidia-nccl-cu12 (307.4MiB)
Downloading hf-xet (3.2MiB)
Downloading uvloop (3.6MiB)
Downloading numpy (16.2MiB)
Downloading pydantic-core (2.0MiB)
 Downloaded pydantic-core
 Downloaded hf-xet
 Downloaded uvloop
 Downloaded numpy
 Downloaded scipy
 Downloaded nvidia-nvshmem-cu12
 Downloaded triton
 Downloaded nvidia-nccl-cu12
 Downloaded torch
Installed 70 packages in 531ms
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0
rootdir: /home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/tts
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 5 items

tests/test_cli.py ...                                                    [ 60%]
tests/test_runner_api.py ..                                              [100%]

============================== 5 passed in 0.28s ===============================
cd apps/backend/services/orchestrator && uv run --with pytest --with pytest-asyncio pytest
Using CPython 3.11.14 interpreter at: /opt/hostedtoolcache/Python/3.11.14/x64/bin/python3.11
Creating virtual environment at: .venv
Downloading soundfile (1.3MiB)
Downloading ml-dtypes (4.7MiB)
Downloading torchaudio (3.8MiB)
Downloading onnxruntime (16.5MiB)
Downloading samplerate (3.8MiB)
Downloading numba (3.6MiB)
Downloading onnx-weekly (17.2MiB)
Downloading llvmlite (53.7MiB)
Downloading pillow (6.3MiB)
Downloading cython (3.2MiB)
Downloading torchvision (8.2MiB)
Downloading resampy (2.9MiB)
Downloading yt-dlp (3.1MiB)
   Building julius==0.2.7
   Building diffq==0.2.4
 Downloaded soundfile
 Downloaded resampy
 Downloaded cython
 Downloaded samplerate
 Downloaded torchaudio
 Downloaded numba
 Downloaded ml-dtypes
      Built julius==0.2.7
 Downloaded pillow
 Downloaded yt-dlp
 Downloaded torchvision
 Downloaded onnxruntime
 Downloaded llvmlite
 Downloaded onnx-weekly
      Built diffq==0.2.4
Installed 106 packages in 382ms
Installed 7 packages in 8ms
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0
rootdir: /home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator
configfile: pyproject.toml
plugins: asyncio-1.3.0, anyio-4.11.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items

tests/test_pipeline_integration.py FF                                    [100%]

=================================== FAILURES ===================================
__________________________ test_dub_pipeline_minimal ___________________________

video_url = '/tmp/pytest-of-runner/pytest-3/test_dub_pipeline_minimal0/input.wav'
target_work = 'dub', target_langs = ['fr'], source_lang = 'en'
min_speakers = None, max_speakers = None
sep_model = 'melband_roformer_big_beta5e.ckpt', asr_model = 'whisperx'
tr_model = 'facebook_m2m100', tts_model = 'chatterbox', audio_sep = False
perform_vad_trimming = False, translation_strategy = 'default'
dubbing_strategy = 'default', sophisticated_dub_timing = True
subtitle_style = None, persist_intermediate = False, involve_mode = False
run_id = None

    @app.post("/v1/dub")
    async def dub(
        video_url: str,
        target_work: str = Query(
            "dub",
            description="Target work type, e.g., 'dub': for full dubbing or 'sub': for subtitles only",
        ),
        target_langs: Optional[List[str] | str] = Query(None),
        source_lang: Optional[str] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        sep_model: str = Query("melband_roformer_big_beta5e.ckpt"),
        asr_model: str = Query("whisperx"),
        tr_model: str = Query("facebook_m2m100"),
        tts_model: str = Query("chatterbox"),
        audio_sep: bool = Query(True, description="Whether to perform audio source separation"),
        perform_vad_trimming: bool = Query(True, description="Whether to perform VAD-based silence trimming after TTS"),
        translation_strategy: str = Query(
            "default",
            description="Translation strategy to use: either translate directly over the short ASR aligned segments or translate the full text and then align the translated result after",
        ),
        dubbing_strategy: str = Query(
            "default",
            description="Dubbing strategy to use, either translation over (original audio ducked) or full replacement",
        ),
        sophisticated_dub_timing: bool = Query(
            True,
            description="Whether to use sophisticated timing for full replacement dubbing strategy",
        ),
        subtitle_style: Optional[str] = Query(
            None,
            description="Subtitle style preset: default, minimal, bold, netflix",
        ),
        persist_intermediate: bool = Query(
            True,
            description="Persist intermediate artifacts (disable for lower latency and disk usage)",
        ),
        involve_mode: bool = Query(
            False,
            description="Enable involve-mode workflow with manual transcription review between stages.",
        ),
        run_id: Optional[str] = Query(
            None,
            description="Optional run identifier when invoked from the job runner (required for involve mode).",
        ),
    ):
        """
        Complete dubbing pipeline orchestrator.
        """
        if involve_mode and not run_id:
            raise HTTPException(400, "Involve mode requires an active run context (run_id).")
    
        original_source = video_url
        video_url = video_url.strip()
        if target_work == "sub" and subtitle_style is None:
            subtitle_style = "default_mobile"
        if dubbing_strategy != "translation_over":
            dubbing_strategy = "full_replacement" # other values default to full_replacement
    
        source_lang = (source_lang or "").strip() or None
        target_languages = normalize_language_codes((list(target_langs)) if target_langs else [])
    
        sep_model = (sep_model or "").strip()
    
        requested_tr_model = (tr_model or "").strip()
        requested_tts_model = (tts_model or "").strip()
    
        asr_model = resolve_model_choice(asr_model, ASR_WORKERS, source_lang, fallback= general_cfg.get("default_models", {}).get("asr", "whisperx"))
    
        translation_models_by_lang: Dict[str, str] = {}
        tts_models_by_lang: Dict[str, str] = {}
        per_language_models: Dict[str, Dict[str, str]] = {}
        for lang in target_languages:
            translation_models_by_lang[lang] = resolve_model_choice(
                requested_tr_model,
                TR_WORKERS,
                lang or source_lang,
                fallback= general_cfg.get("default_models", {}).get("tr", "deep_translator"),
            )
            tts_models_by_lang[lang] = resolve_model_choice(
                requested_tts_model,
                TTS_WORKERS,
                lang,
                fallback= general_cfg.get("default_models", {}).get("tts", "chatterbox"),
            )
    
        selected_models = {
            "asr": asr_model,
            "translation": translation_models_by_lang,
            "tts": tts_models_by_lang,
            "separation": sep_model,
        }
    
        if translation_strategy not in TRANSLATION_STRATEGIES:
            translation_strategy = TRANSLATION_STRATEGIES[0]
        if dubbing_strategy not in DUBBING_STRATEGIES:
            dubbing_strategy = DUBBING_STRATEGIES[0]
    
        workspace = WorkspaceManager.create(OUTS, persist_intermediate)
        step_timer = StepTimer()
        client = get_http_client()
    
        subtitles_dir: Optional[Path] = None
    
        resolved_video_path, media_digest = await prepare_media_source(video_url, workspace)
        source_media_local_path: Optional[Path] = resolved_video_path
    
        source_has_video = await has_video_stream(resolved_video_path) # better check to avoid issues with audio-only inputs being treated as videos
    
        if not source_has_video:
            target_work = "dub"
            subtitle_style = None
            dubbing_strategy = "full_replacement"
    
        if workspace.persist_intermediate:
            preprocessing_dir = workspace.ensure_dir("preprocessing")
        else:
            preprocessing_dir = workspace.make_temp_dir("preprocessing")
    
        raw_audio_path = preprocessing_dir / "raw_audio.wav"
    
        cancelled = False
    
        try:
            raw_audio_cached = False
            raw_audio_cache_token: Optional[str] = None
            if media_digest:
                raw_audio_cache_token = raw_audio_cache_key(media_digest)
                raw_audio_cached = await load_cached_raw_audio(raw_audio_cache_token, raw_audio_path)
                if raw_audio_cached:
                    logger.info("Loaded raw audio from cache for media digest %s", media_digest)
    
            if not raw_audio_cached:
                with step_timer.time("extract_audio"):
                    await extract_audio_to_workspace(str(resolved_video_path), raw_audio_path)
                if raw_audio_cache_token:
                    await store_raw_audio_cache(raw_audio_cache_token, raw_audio_path)
    
            raw_audio_duration = get_audio_duration(raw_audio_path)
    
            vocals_path: Optional[Path] = None
            background_path: Optional[Path] = None
    
            vocal_for_transcript = general_cfg.get("vocal_only_for_transcription", True)
            logger.info("Vocal only for transcription: %s", vocal_for_transcript)
    
            if target_work != "sub" or vocal_for_transcript: # in subtitle-only mode, no need to separate audio for now: in future we might want to do it for better ASR performance if we succeed to implement automatic noise level detection
                with step_timer.time("audio_separation"):
    
                    vocals_path, background_path, dubbing_strategy = await maybe_run_audio_separation(
                        preprocessing_dir,
                        raw_audio_path,
                        sep_model,
                        audio_sep,
                        dubbing_strategy,
                    )
    
            transcript_audio = vocals_path if vocals_path and vocal_for_transcript else raw_audio_path
    
            with step_timer.time("asr"):
                raw_asr_result, aligned_asr_result = await run_asr_step(
                    client,
                    transcript_audio,
                    asr_model,
                    source_lang,
                    min_speakers,
                    max_speakers,
                    perform_alignment=not involve_mode,
                )
    
            original_raw_dump = raw_asr_result.model_dump()
            asr_raw_path = ""
    
            if involve_mode:
                languages_set: set[str] = set()
                for worker in ASR_WORKERS.values():
                    worker_langs = getattr(worker, "languages", None)
                    if worker_langs:
                        for lang in worker_langs:
                            normalized = (lang or "").strip().lower()
                            if normalized:
                                languages_set.add(normalized)
                for segment in raw_asr_result.segments:
                    normalized = (segment.lang or "").strip().lower()
                    if normalized:
                        languages_set.add(normalized)
                available_languages = sorted(languages_set)
                TRANSCRIPTION_REVIEW_SESSIONS[run_id] = TranscriptionReviewSession(
                    run_id=run_id,
                    audio_duration=raw_audio_duration,
                    audio_path=raw_asr_result.audio_url or str(raw_audio_path),
                    languages=available_languages,
                    tolerance=TRANSCRIPTION_SEGMENT_TOLERANCE,
                )
                emit_progress({
                    "type": "transcription_review",
                    "run_id": run_id,
                    "raw": original_raw_dump,
                    "languages": available_languages,
                    "duration": raw_audio_duration,
                    "tolerance": TRANSCRIPTION_SEGMENT_TOLERANCE,
                    "artifacts": {"raw_path": asr_raw_path},
                })
                emit_progress({"type": "status", "event": "awaiting_transcription_review"})
                reviewed_result = await wait_for_transcription_review(run_id)
    
                merged_extra = dict(raw_asr_result.extra or {})
                merged_extra.update(reviewed_result.extra or {})
                reviewed_result.extra = merged_extra
                if not reviewed_result.audio_url:
                    reviewed_result.audio_url = raw_asr_result.audio_url or str(raw_audio_path)
                if not reviewed_result.language:
                    reviewed_result.language = raw_asr_result.language
    
                reviewed_result.segments = sorted(
                    reviewed_result.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
    
                raw_asr_result = reviewed_result
                raw_asr_result.extra.setdefault("enable_diarization", True)
                aligned_asr_result = await align_asr_transcription(
                    client,
                    asr_model,
                    raw_asr_result,
                    diarize=True,
                )
                emit_progress({"type": "transcription_review_complete", "run_id": run_id})
    
            asr_raw_path = workspace.maybe_dump_json("asr/asr_0_result.json", raw_asr_result.model_dump())
    
            source_lang = raw_asr_result.language or source_lang
    
    
            if aligned_asr_result is None:
                raise HTTPException(500, "ASR alignment result missing after transcription stage.")
    
            initial_aligned_dump = aligned_asr_result.model_dump()
            asr_aligned_path = ""
    
            if involve_mode:
                speakers = sorted({seg.speaker_id for seg in aligned_asr_result.segments if seg.speaker_id})
                emit_progress(
                    {
                        "type": "alignment_review",
                        "run_id": run_id,
                        "aligned": initial_aligned_dump,
                        "speakers": speakers,
                        "artifacts": {
                            "aligned_path": asr_aligned_path,
                            "raw_path": asr_raw_path,
                        },
                    }
                )
                emit_progress({"type": "status", "event": "awaiting_alignment_review"})
                reviewed_alignment = await wait_for_alignment_review(run_id)
    
                merged_extra = dict(aligned_asr_result.extra or {})
                merged_extra.update(reviewed_alignment.extra or {})
                reviewed_alignment.extra = merged_extra
                if not reviewed_alignment.audio_url:
                    reviewed_alignment.audio_url = aligned_asr_result.audio_url or raw_asr_result.audio_url
                if not reviewed_alignment.language:
                    reviewed_alignment.language = aligned_asr_result.language or raw_asr_result.language
    
                reviewed_alignment.segments = sorted(
                    reviewed_alignment.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
                aligned_asr_result = reviewed_alignment
                emit_progress({"type": "alignment_review_complete", "run_id": run_id})
    
            asr_aligned_path = workspace.maybe_dump_json(
                "asr/asr_0_aligned_result.json",
                aligned_asr_result.model_dump(),
            )
    
            srt_path_0 = ""
            vtt_path_0 = ""
            subtitles_dir: Optional[Path] = None
    
            if subtitle_style is not None:
                with step_timer.time("subtitles_original"):
    
                    if workspace.persist_intermediate:
                        subtitles_dir = workspace.ensure_dir("subtitles")
                    else:
                        subtitles_dir = workspace.make_temp_dir("subtitles")
    
                    srt_path_0, vtt_path_0 = build_subtitles_from_asr_result(
                        data=aligned_asr_result.model_dump(),
                        output_dir=subtitles_dir,
                        custom_name="original",
                        formats=["srt", "vtt"],
                        mobile_mode=subtitle_style.split("_")[-1] == "mobile",
                    )
    
            translation_mode = translation_strategy.split("_")[0]
            segments_for_translation = (
                raw_asr_result.model_dump()["segments"]
                if translation_mode == "long"
                else aligned_asr_result.model_dump()["segments"]
            )
    
            languages_to_process = target_languages
            default_language = target_languages[0] if target_languages else (source_lang if target_work == "sub" else None)
    
            subtitles_per_language: Dict[str, Dict[str, Dict[str, str]]] = {}
            language_payloads: Dict[str, Dict[str, Any]] = {}
    
            srt_path_1 = srt_path_0
            vtt_path_1 = vtt_path_0
            final_video_path = str(resolved_video_path) if source_has_video else ""
            default_audio_path = ""
            default_speech_track = ""
    
            subtitle_style_prefix = subtitle_style.split("_")[0] if subtitle_style is not None else ""
            subtitle_mobile_mode = subtitle_style.split("_")[-1] == "mobile" if subtitle_style is not None else False
            style = STYLE_PRESETS.get(subtitle_style_prefix, STYLE_PRESETS["default"]) if subtitle_style is not None else None
    
            if target_work == "sub":
                # subtitles-only
                if languages_to_process:
                    for lang in languages_to_process:
                        with step_timer.time(f"translation[{lang}]"):
                            tr_result = await translate_segments(
                                client,
                                translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100")),
                                tr_provider,  # ADD: provider
                                segments_for_translation,
                                source_lang,
                                lang,
                            )
                        workspace.maybe_dump_json(
                            f"translation/{lang}/translation_result.json",
                            tr_result.model_dump(),
                        )
    
                        if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                            with step_timer.time(f"translation_alignment[{lang}]"):
                                tr_result = await align_translation_segments(
                                    tr_result,
                                    raw_asr_result,
                                    aligned_asr_result,
                                    translation_strategy,
                                    lang,
                                )
                            workspace.maybe_dump_json(
                                f"translation/{lang}/translation_aligned_W_origin_result.json",
                                tr_result.model_dump(),
                            )
    
                        if subtitle_style is not None and subtitles_dir:
                            trans_srt, trans_vtt = build_subtitles_from_asr_result(
                                data=tr_result.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                            subtitles_per_language[lang] = {"aligned": {"srt": trans_srt, "vtt": trans_vtt}}
    
                    if default_language and default_language in subtitles_per_language:
                        align = subtitles_per_language[default_language]["aligned"]
                        srt_path_1 = align.get("srt", srt_path_1)
                        vtt_path_1 = align.get("vtt", vtt_path_1)
    
                dubbed_path = resolved_video_path
                final_output = (
                    workspace.file_path(f"subtitled_video_{default_language or source_lang}_with_{subtitle_style_prefix}_subs.mp4")
                    if subtitle_style is not None
                    else None
                )
    
                with step_timer.time("final_pass"):
                    await finalize_media(
                        str(resolved_video_path),
                        None,
                        dubbed_path,
                        final_output,
                        Path(vtt_path_1) if vtt_path_1 else None,
                        style,
                        subtitle_mobile_mode,
                        dubbing_strategy,
                    )
    
                final_video_path = str(final_output) if final_output else str(dubbed_path)
            else:
                # dubbing
                if not languages_to_process:
                    raise HTTPException(400, "At least one target language must be specified for dubbing")
    
                async def process_language(lang: str) -> Tuple[str, Dict[str, Any]]:
                    lang_suffix = f"[{lang}]"
                    translation_model_key = translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100"))
                    tts_model_key = tts_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tts", "chatterbox"))
                    per_language_models[lang] = {"translation": translation_model_key, "tts": tts_model_key}
    
                    with step_timer.time(f"translation{lang_suffix}"):
                        tr_result = await translate_segments(
                            client,
                            translation_model_key,
                            tr_provider,  # ADD: provider
                            segments_for_translation,
                            source_lang,
                            lang,
                        )
    
    
                    if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                        with step_timer.time(f"translation_alignment{lang_suffix}"):
                            tr_result = await align_translation_segments(
                                tr_result,
                                raw_asr_result,
                                aligned_asr_result,
                                translation_strategy,
                                lang,
                            )
                        tr_aligned_origin_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_origin_result.json",
                            tr_result.model_dump(),
                        )
                    else:
                        tr_aligned_origin_path = ""
    
                    ensure_segment_ids(tr_result)
    
                    tr_result.audio_url = str(vocals_path) if vocals_path else str(raw_audio_path) # use vocal if available because it's cleaner for cloning
    
                    if workspace.persist_intermediate:
                        prompt_audio_dir = workspace.ensure_dir(f"prompts/{lang}")
                    else:
                        prompt_audio_dir = workspace.make_temp_dir(f"prompts_{lang}")
                    with step_timer.time(f"prompt_attachment{lang_suffix}"):
                        updated = await run_in_thread(
                            attach_segment_audio_clips,
                            asr_dump=tr_result.model_dump(),
                            output_dir=prompt_audio_dir,
                            min_duration= general_cfg.get("prompt_attachment", {}).get("min_duration", 1.0),
                            max_duration= general_cfg.get("prompt_attachment", {}).get("max_duration", 40.0),
                            one_per_speaker=True,
                        )
                    tr_result_local = ASRResponse(**updated)
    
                    if workspace.persist_intermediate:
                        tts_output_dir = workspace.ensure_dir(f"tts/{lang}")
                    else:
                        tts_output_dir = workspace.make_temp_dir(f"tts_{lang}")
                    with step_timer.time(f"tts{lang_suffix}"):
                        tts_result = await synthesize_tts(client, tts_model_key, tr_result_local, lang, tts_output_dir)
    
    
                    if involve_mode:
                        await run_tts_review_session(
                            run_id=run_id,
                            language=lang,
                            tts_model=tts_model_key,
                            workspace_path=tts_output_dir,
                            translation=tr_result_local,
                            tts_result=tts_result,
                        )
    
                    tr_out_path = workspace.maybe_dump_json(
                        f"translation/{lang}/translation_result.json",
                        tr_result_local.model_dump(),
                    )
    
                    tts_out_path = workspace.maybe_dump_json(
                        f"tts/{lang}/tts_result.json",
                        tts_result.model_dump(),
                    )
    
                    if perform_vad_trimming:
                        if workspace.persist_intermediate:
                            vad_dir = workspace.ensure_dir(f"vad_trimmed/{lang}")
                        else:
                            vad_dir = workspace.make_temp_dir(f"vad_trimmed_{lang}")
                        with step_timer.time(f"tts_vad_trim{lang_suffix}"):
                            tts_result = await trim_tts_segments(tts_result, vad_dir)
                        workspace.maybe_dump_json(
                            f"tts/{lang}/tts_result.json",
                            tts_result.model_dump(),
                        )
    
                    if workspace.persist_intermediate or not source_has_video:
                        audio_processing_dir = workspace.ensure_dir(f"audio_processing/{lang}")
                    else:
                        audio_processing_dir = workspace.make_temp_dir(f"audio_processing_{lang}")
    
    
                    speech_track = audio_processing_dir / f"dubbed_speech_track_{lang}.wav"
                    with step_timer.time(f"audio_concatenate{lang_suffix}"):
                        concatenated_path, translation_segments = await concatenate_segments(
                            tts_result.model_dump()["segments"],
                            speech_track,
                            target_duration=raw_audio_duration,
                            translation_segments=tr_result_local.model_dump()["segments"],
                        )
                    final_audio_path = Path(concatenated_path)
    
                    if dubbing_strategy == "full_replacement" and background_path:
                        with step_timer.time(f"audio_overlay{lang_suffix}"):
                            final_audio_path = audio_processing_dir / f"final_dubbed_audio_{lang}.wav"
                            await overlay_segments_on_background(
                                tts_result.model_dump()["segments"],
                                background_path=background_path,
                                output_path=final_audio_path,
                                sophisticated=sophisticated_dub_timing,
                                speech_track=speech_track,
                            )
                    else:
                        logger.info("Using translation-over dubbing strategy for language %s", lang)
    
                    aligned_srt = ""
                    aligned_vtt = ""
                    if subtitle_style is not None:
                        with step_timer.time(f"dubbed_alignment{lang_suffix}"):
                            aligned_tts = await align_dubbed_audio(
                                client,
                                asr_model,
                                tr_result_local,
                                translation_segments,
                                final_audio_path,
                            )
                        tr_aligned_tts_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_dubbedvoice_result.json",
                            aligned_tts.model_dump(),
                        )
                        if subtitles_dir:
                            aligned_srt, aligned_vtt = build_subtitles_from_asr_result(
                                data=aligned_tts.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                    else:
                        tr_aligned_tts_path = ""
    
                    final_video_out: str = ""
                    if source_has_video:
                        dubbed_path = workspace.file_path(f"dubbed_video_{lang}.mp4")
                        final_output = (
                            workspace.file_path(f"dubbed_video_{lang}_with_{subtitle_style_prefix}_subs.mp4")
                            if subtitle_style is not None
                            else None
                        )
                        with step_timer.time(f"final_pass[{lang}]"):
                            await finalize_media(
                                str(resolved_video_path),
                                final_audio_path,
                                dubbed_path,
                                final_output,
                                Path(aligned_vtt) if aligned_vtt else None,
                                style,
                                subtitle_mobile_mode,
                                dubbing_strategy,
                            )
                        final_video_out = str(final_output) if final_output else str(dubbed_path)
                    else:
                        final_video_out = str(final_audio_path)
    
                    payload = {
                        "final_video_path": final_video_out,
                        "final_audio_path": str(final_audio_path) if final_audio_path else "",
                        "speech_track": str(speech_track),
                        "subtitles": {"aligned": {"srt": aligned_srt, "vtt": aligned_vtt}},
                        "intermediate_files": {
                            "translation": tr_out_path,
                            "translation_aligned_W_origin": tr_aligned_origin_path,
                            "translation_aligned_W_dubbedvoice": tr_aligned_tts_path,
                            "tts": tts_out_path,
                        },
                        "models": {"translation": translation_model_key, "tts": tts_model_key},
                    }
                    return lang, payload
    
>               results = await asyncio.gather(*(process_language(lang) for lang in languages_to_process))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

app/main.py:2359: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

lang = 'fr'

    async def process_language(lang: str) -> Tuple[str, Dict[str, Any]]:
        lang_suffix = f"[{lang}]"
        translation_model_key = translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100"))
        tts_model_key = tts_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tts", "chatterbox"))
        per_language_models[lang] = {"translation": translation_model_key, "tts": tts_model_key}
    
        with step_timer.time(f"translation{lang_suffix}"):
            tr_result = await translate_segments(
                client,
                translation_model_key,
>               tr_provider,  # ADD: provider
                ^^^^^^^^^^^
                segments_for_translation,
                source_lang,
                lang,
            )
E           NameError: name 'tr_provider' is not defined

app/main.py:2184: NameError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc1aefc95d0>
tmp_path = PosixPath('/tmp/pytest-of-runner/pytest-3/test_dub_pipeline_minimal0')

    @pytest.mark.asyncio
    async def test_dub_pipeline_minimal(monkeypatch, tmp_path):
        # Create a 1-second silent wav input to avoid ffmpeg dependency.
        input_wav = tmp_path / "input.wav"
        with wave.open(str(input_wav), "w") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(b"\x00\x00" * 16000)
    
        original_outs = orchestrator_main.OUTS
        orchestrator_main.OUTS = tmp_path
    
        async def fake_maybe_run_audio_separation(*args, **kwargs):  # noqa: ANN001
            return None, None, "default"
    
        async def fake_run_asr_step(*args, **kwargs):  # noqa: ANN001
            seg = Segment(start=0.0, end=1.0, text="Hello", speaker_id="spk1", lang="en")
            response = ASRResponse(segments=[seg], language="en")
            return response, response
    
        async def fake_run_translation_step(*args, **kwargs):  # noqa: ANN001
            seg = Segment(start=0.0, end=1.0, text="Bonjour", speaker_id="spk1", lang="fr")
            return ASRResponse(segments=[seg], language="fr")
    
        tts_audio = tmp_path / "tts.wav"
        with wave.open(str(tts_audio), "w") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(b"\x00\x00" * 16000)
    
        async def fake_synthesize_tts(*args, **kwargs):  # noqa: ANN001
            segment_out = SegmentAudioOut(
                start=0.0,
                end=1.0,
                audio_url=str(tts_audio),
                speaker_id="spk1",
                lang="fr",
                sample_rate=16000,
            )
            return TTSResponse(segments=[segment_out])
    
        async def fake_trim_tts_segments(tts_result, vad_dir):  # noqa: ANN001
            return tts_result
    
        async def fake_concatenate_segments(*args, **kwargs):  # noqa: ANN001
            speech_track = tmp_path / "speech.wav"
            speech_track.write_bytes(tts_audio.read_bytes())
            return str(speech_track), [{"start": 0.0, "end": 1.0, "text": "Bonjour", "speaker_id": "spk1"}]
    
        async def fake_align_dubbed_audio(*args, **kwargs):  # noqa: ANN001
            seg = Segment(start=0.0, end=1.0, text="Bonjour", speaker_id="spk1", lang="fr")
            return ASRResponse(segments=[seg], language="fr")
    
        async def fake_finalize_media(
            video_path,
            audio_path,
            dubbed_path,
            output_path,
            subtitle_path,
            sub_style,
            mobile_optimized,
            dubbing_strategy,
        ):  # noqa: ANN001
            Path(dubbed_path).write_text("video")
            if output_path:
                Path(output_path).write_text("video with subtitles")
    
        monkeypatch.setattr(orchestrator_main, "maybe_run_audio_separation", fake_maybe_run_audio_separation)
        monkeypatch.setattr(orchestrator_main, "run_asr_step", fake_run_asr_step)
        monkeypatch.setattr(orchestrator_main, "translate_segments", fake_run_translation_step)
        monkeypatch.setattr(orchestrator_main, "synthesize_tts", fake_synthesize_tts)
        monkeypatch.setattr(orchestrator_main, "trim_tts_segments", fake_trim_tts_segments)
        monkeypatch.setattr(orchestrator_main, "concatenate_segments", fake_concatenate_segments)
        monkeypatch.setattr(orchestrator_main, "align_dubbed_audio", fake_align_dubbed_audio)
        monkeypatch.setattr(orchestrator_main, "finalize_media", fake_finalize_media)
        monkeypatch.setattr(orchestrator_main, "get_audio_duration", lambda *args, **kwargs: 1.0)  # noqa: ARG005
    
        await orchestrator_main.startup_event()
        try:
>           result = await orchestrator_main.dub(
                video_url=str(input_wav),
                target_work="dub",
                target_langs=["fr"],
                source_lang="en",
                translation_strategy="default",
                dubbing_strategy="default",
                sophisticated_dub_timing=True,
                subtitle_style="default_mobile",
                audio_sep=False,
                perform_vad_trimming=False,
                persist_intermediate=False,
                sep_model="melband_roformer_big_beta5e.ckpt",
                asr_model="whisperx",
                tr_model="facebook_m2m100",
                tts_model="chatterbox",
                run_id=None,
                involve_mode=False,
            )

tests/test_pipeline_integration.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

video_url = '/tmp/pytest-of-runner/pytest-3/test_dub_pipeline_minimal0/input.wav'
target_work = 'dub', target_langs = ['fr'], source_lang = 'en'
min_speakers = None, max_speakers = None
sep_model = 'melband_roformer_big_beta5e.ckpt', asr_model = 'whisperx'
tr_model = 'facebook_m2m100', tts_model = 'chatterbox', audio_sep = False
perform_vad_trimming = False, translation_strategy = 'default'
dubbing_strategy = 'default', sophisticated_dub_timing = True
subtitle_style = None, persist_intermediate = False, involve_mode = False
run_id = None

    @app.post("/v1/dub")
    async def dub(
        video_url: str,
        target_work: str = Query(
            "dub",
            description="Target work type, e.g., 'dub': for full dubbing or 'sub': for subtitles only",
        ),
        target_langs: Optional[List[str] | str] = Query(None),
        source_lang: Optional[str] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        sep_model: str = Query("melband_roformer_big_beta5e.ckpt"),
        asr_model: str = Query("whisperx"),
        tr_model: str = Query("facebook_m2m100"),
        tts_model: str = Query("chatterbox"),
        audio_sep: bool = Query(True, description="Whether to perform audio source separation"),
        perform_vad_trimming: bool = Query(True, description="Whether to perform VAD-based silence trimming after TTS"),
        translation_strategy: str = Query(
            "default",
            description="Translation strategy to use: either translate directly over the short ASR aligned segments or translate the full text and then align the translated result after",
        ),
        dubbing_strategy: str = Query(
            "default",
            description="Dubbing strategy to use, either translation over (original audio ducked) or full replacement",
        ),
        sophisticated_dub_timing: bool = Query(
            True,
            description="Whether to use sophisticated timing for full replacement dubbing strategy",
        ),
        subtitle_style: Optional[str] = Query(
            None,
            description="Subtitle style preset: default, minimal, bold, netflix",
        ),
        persist_intermediate: bool = Query(
            True,
            description="Persist intermediate artifacts (disable for lower latency and disk usage)",
        ),
        involve_mode: bool = Query(
            False,
            description="Enable involve-mode workflow with manual transcription review between stages.",
        ),
        run_id: Optional[str] = Query(
            None,
            description="Optional run identifier when invoked from the job runner (required for involve mode).",
        ),
    ):
        """
        Complete dubbing pipeline orchestrator.
        """
    
        if involve_mode and not run_id:
            raise HTTPException(400, "Involve mode requires an active run context (run_id).")
    
        original_source = video_url
        video_url = video_url.strip()
        if target_work == "sub" and subtitle_style is None:
            subtitle_style = "default_mobile"
        if dubbing_strategy != "translation_over":
            dubbing_strategy = "full_replacement" # other values default to full_replacement
    
        source_lang = (source_lang or "").strip() or None
        target_languages = normalize_language_codes((list(target_langs)) if target_langs else [])
    
        sep_model = (sep_model or "").strip()
    
        requested_tr_model = (tr_model or "").strip()
        requested_tts_model = (tts_model or "").strip()
    
        asr_model = resolve_model_choice(asr_model, ASR_WORKERS, source_lang, fallback= general_cfg.get("default_models", {}).get("asr", "whisperx"))
    
        translation_models_by_lang: Dict[str, str] = {}
        tts_models_by_lang: Dict[str, str] = {}
        per_language_models: Dict[str, Dict[str, str]] = {}
        for lang in target_languages:
            translation_models_by_lang[lang] = resolve_model_choice(
                requested_tr_model,
                TR_WORKERS,
                lang or source_lang,
                fallback= general_cfg.get("default_models", {}).get("tr", "deep_translator"),
            )
            tts_models_by_lang[lang] = resolve_model_choice(
                requested_tts_model,
                TTS_WORKERS,
                lang,
                fallback= general_cfg.get("default_models", {}).get("tts", "chatterbox"),
            )
    
        selected_models = {
            "asr": asr_model,
            "translation": translation_models_by_lang,
            "tts": tts_models_by_lang,
            "separation": sep_model,
        }
    
        if translation_strategy not in TRANSLATION_STRATEGIES:
            translation_strategy = TRANSLATION_STRATEGIES[0]
        if dubbing_strategy not in DUBBING_STRATEGIES:
            dubbing_strategy = DUBBING_STRATEGIES[0]
    
        workspace = WorkspaceManager.create(OUTS, persist_intermediate)
        step_timer = StepTimer()
        client = get_http_client()
    
        subtitles_dir: Optional[Path] = None
    
        resolved_video_path, media_digest = await prepare_media_source(video_url, workspace)
        source_media_local_path: Optional[Path] = resolved_video_path
    
        source_has_video = await has_video_stream(resolved_video_path) # better check to avoid issues with audio-only inputs being treated as videos
    
        if not source_has_video:
            target_work = "dub"
            subtitle_style = None
            dubbing_strategy = "full_replacement"
    
        if workspace.persist_intermediate:
            preprocessing_dir = workspace.ensure_dir("preprocessing")
        else:
            preprocessing_dir = workspace.make_temp_dir("preprocessing")
    
        raw_audio_path = preprocessing_dir / "raw_audio.wav"
    
        cancelled = False
    
        try:
            raw_audio_cached = False
            raw_audio_cache_token: Optional[str] = None
            if media_digest:
                raw_audio_cache_token = raw_audio_cache_key(media_digest)
                raw_audio_cached = await load_cached_raw_audio(raw_audio_cache_token, raw_audio_path)
                if raw_audio_cached:
                    logger.info("Loaded raw audio from cache for media digest %s", media_digest)
    
            if not raw_audio_cached:
                with step_timer.time("extract_audio"):
                    await extract_audio_to_workspace(str(resolved_video_path), raw_audio_path)
                if raw_audio_cache_token:
                    await store_raw_audio_cache(raw_audio_cache_token, raw_audio_path)
    
            raw_audio_duration = get_audio_duration(raw_audio_path)
    
            vocals_path: Optional[Path] = None
            background_path: Optional[Path] = None
    
            vocal_for_transcript = general_cfg.get("vocal_only_for_transcription", True)
            logger.info("Vocal only for transcription: %s", vocal_for_transcript)
    
            if target_work != "sub" or vocal_for_transcript: # in subtitle-only mode, no need to separate audio for now: in future we might want to do it for better ASR performance if we succeed to implement automatic noise level detection
                with step_timer.time("audio_separation"):
    
                    vocals_path, background_path, dubbing_strategy = await maybe_run_audio_separation(
                        preprocessing_dir,
                        raw_audio_path,
                        sep_model,
                        audio_sep,
                        dubbing_strategy,
                    )
    
            transcript_audio = vocals_path if vocals_path and vocal_for_transcript else raw_audio_path
    
            with step_timer.time("asr"):
                raw_asr_result, aligned_asr_result = await run_asr_step(
                    client,
                    transcript_audio,
                    asr_model,
                    source_lang,
                    min_speakers,
                    max_speakers,
                    perform_alignment=not involve_mode,
                )
    
            original_raw_dump = raw_asr_result.model_dump()
            asr_raw_path = ""
    
            if involve_mode:
                languages_set: set[str] = set()
                for worker in ASR_WORKERS.values():
                    worker_langs = getattr(worker, "languages", None)
                    if worker_langs:
                        for lang in worker_langs:
                            normalized = (lang or "").strip().lower()
                            if normalized:
                                languages_set.add(normalized)
                for segment in raw_asr_result.segments:
                    normalized = (segment.lang or "").strip().lower()
                    if normalized:
                        languages_set.add(normalized)
                available_languages = sorted(languages_set)
                TRANSCRIPTION_REVIEW_SESSIONS[run_id] = TranscriptionReviewSession(
                    run_id=run_id,
                    audio_duration=raw_audio_duration,
                    audio_path=raw_asr_result.audio_url or str(raw_audio_path),
                    languages=available_languages,
                    tolerance=TRANSCRIPTION_SEGMENT_TOLERANCE,
                )
                emit_progress({
                    "type": "transcription_review",
                    "run_id": run_id,
                    "raw": original_raw_dump,
                    "languages": available_languages,
                    "duration": raw_audio_duration,
                    "tolerance": TRANSCRIPTION_SEGMENT_TOLERANCE,
                    "artifacts": {"raw_path": asr_raw_path},
                })
                emit_progress({"type": "status", "event": "awaiting_transcription_review"})
                reviewed_result = await wait_for_transcription_review(run_id)
    
                merged_extra = dict(raw_asr_result.extra or {})
                merged_extra.update(reviewed_result.extra or {})
                reviewed_result.extra = merged_extra
                if not reviewed_result.audio_url:
                    reviewed_result.audio_url = raw_asr_result.audio_url or str(raw_audio_path)
                if not reviewed_result.language:
                    reviewed_result.language = raw_asr_result.language
    
                reviewed_result.segments = sorted(
                    reviewed_result.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
    
                raw_asr_result = reviewed_result
                raw_asr_result.extra.setdefault("enable_diarization", True)
                aligned_asr_result = await align_asr_transcription(
                    client,
                    asr_model,
                    raw_asr_result,
                    diarize=True,
                )
                emit_progress({"type": "transcription_review_complete", "run_id": run_id})
    
            asr_raw_path = workspace.maybe_dump_json("asr/asr_0_result.json", raw_asr_result.model_dump())
    
            source_lang = raw_asr_result.language or source_lang
    
    
            if aligned_asr_result is None:
                raise HTTPException(500, "ASR alignment result missing after transcription stage.")
    
            initial_aligned_dump = aligned_asr_result.model_dump()
            asr_aligned_path = ""
    
            if involve_mode:
                speakers = sorted({seg.speaker_id for seg in aligned_asr_result.segments if seg.speaker_id})
                emit_progress(
                    {
                        "type": "alignment_review",
                        "run_id": run_id,
                        "aligned": initial_aligned_dump,
                        "speakers": speakers,
                        "artifacts": {
                            "aligned_path": asr_aligned_path,
                            "raw_path": asr_raw_path,
                        },
                    }
                )
                emit_progress({"type": "status", "event": "awaiting_alignment_review"})
                reviewed_alignment = await wait_for_alignment_review(run_id)
    
                merged_extra = dict(aligned_asr_result.extra or {})
                merged_extra.update(reviewed_alignment.extra or {})
                reviewed_alignment.extra = merged_extra
                if not reviewed_alignment.audio_url:
                    reviewed_alignment.audio_url = aligned_asr_result.audio_url or raw_asr_result.audio_url
                if not reviewed_alignment.language:
                    reviewed_alignment.language = aligned_asr_result.language or raw_asr_result.language
    
                reviewed_alignment.segments = sorted(
                    reviewed_alignment.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
                aligned_asr_result = reviewed_alignment
                emit_progress({"type": "alignment_review_complete", "run_id": run_id})
    
            asr_aligned_path = workspace.maybe_dump_json(
                "asr/asr_0_aligned_result.json",
                aligned_asr_result.model_dump(),
            )
    
            srt_path_0 = ""
            vtt_path_0 = ""
            subtitles_dir: Optional[Path] = None
    
            if subtitle_style is not None:
                with step_timer.time("subtitles_original"):
    
                    if workspace.persist_intermediate:
                        subtitles_dir = workspace.ensure_dir("subtitles")
                    else:
                        subtitles_dir = workspace.make_temp_dir("subtitles")
    
                    srt_path_0, vtt_path_0 = build_subtitles_from_asr_result(
                        data=aligned_asr_result.model_dump(),
                        output_dir=subtitles_dir,
                        custom_name="original",
                        formats=["srt", "vtt"],
                        mobile_mode=subtitle_style.split("_")[-1] == "mobile",
                    )
    
            translation_mode = translation_strategy.split("_")[0]
            segments_for_translation = (
                raw_asr_result.model_dump()["segments"]
                if translation_mode == "long"
                else aligned_asr_result.model_dump()["segments"]
            )
    
            languages_to_process = target_languages
            default_language = target_languages[0] if target_languages else (source_lang if target_work == "sub" else None)
    
            subtitles_per_language: Dict[str, Dict[str, Dict[str, str]]] = {}
            language_payloads: Dict[str, Dict[str, Any]] = {}
    
            srt_path_1 = srt_path_0
            vtt_path_1 = vtt_path_0
            final_video_path = str(resolved_video_path) if source_has_video else ""
            default_audio_path = ""
            default_speech_track = ""
    
            subtitle_style_prefix = subtitle_style.split("_")[0] if subtitle_style is not None else ""
            subtitle_mobile_mode = subtitle_style.split("_")[-1] == "mobile" if subtitle_style is not None else False
            style = STYLE_PRESETS.get(subtitle_style_prefix, STYLE_PRESETS["default"]) if subtitle_style is not None else None
    
            if target_work == "sub":
                # subtitles-only
                if languages_to_process:
                    for lang in languages_to_process:
                        with step_timer.time(f"translation[{lang}]"):
                            tr_result = await translate_segments(
                                client,
                                translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100")),
                                tr_provider,  # ADD: provider
                                segments_for_translation,
                                source_lang,
                                lang,
                            )
                        workspace.maybe_dump_json(
                            f"translation/{lang}/translation_result.json",
                            tr_result.model_dump(),
                        )
    
                        if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                            with step_timer.time(f"translation_alignment[{lang}]"):
                                tr_result = await align_translation_segments(
                                    tr_result,
                                    raw_asr_result,
                                    aligned_asr_result,
                                    translation_strategy,
                                    lang,
                                )
                            workspace.maybe_dump_json(
                                f"translation/{lang}/translation_aligned_W_origin_result.json",
                                tr_result.model_dump(),
                            )
    
                        if subtitle_style is not None and subtitles_dir:
                            trans_srt, trans_vtt = build_subtitles_from_asr_result(
                                data=tr_result.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                            subtitles_per_language[lang] = {"aligned": {"srt": trans_srt, "vtt": trans_vtt}}
    
                    if default_language and default_language in subtitles_per_language:
                        align = subtitles_per_language[default_language]["aligned"]
                        srt_path_1 = align.get("srt", srt_path_1)
                        vtt_path_1 = align.get("vtt", vtt_path_1)
    
                dubbed_path = resolved_video_path
                final_output = (
                    workspace.file_path(f"subtitled_video_{default_language or source_lang}_with_{subtitle_style_prefix}_subs.mp4")
                    if subtitle_style is not None
                    else None
                )
    
                with step_timer.time("final_pass"):
                    await finalize_media(
                        str(resolved_video_path),
                        None,
                        dubbed_path,
                        final_output,
                        Path(vtt_path_1) if vtt_path_1 else None,
                        style,
                        subtitle_mobile_mode,
                        dubbing_strategy,
                    )
    
                final_video_path = str(final_output) if final_output else str(dubbed_path)
            else:
                # dubbing
                if not languages_to_process:
                    raise HTTPException(400, "At least one target language must be specified for dubbing")
    
                async def process_language(lang: str) -> Tuple[str, Dict[str, Any]]:
                    lang_suffix = f"[{lang}]"
                    translation_model_key = translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100"))
                    tts_model_key = tts_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tts", "chatterbox"))
                    per_language_models[lang] = {"translation": translation_model_key, "tts": tts_model_key}
    
                    with step_timer.time(f"translation{lang_suffix}"):
                        tr_result = await translate_segments(
                            client,
                            translation_model_key,
                            tr_provider,  # ADD: provider
                            segments_for_translation,
                            source_lang,
                            lang,
                        )
    
    
                    if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                        with step_timer.time(f"translation_alignment{lang_suffix}"):
                            tr_result = await align_translation_segments(
                                tr_result,
                                raw_asr_result,
                                aligned_asr_result,
                                translation_strategy,
                                lang,
                            )
                        tr_aligned_origin_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_origin_result.json",
                            tr_result.model_dump(),
                        )
                    else:
                        tr_aligned_origin_path = ""
    
                    ensure_segment_ids(tr_result)
    
                    tr_result.audio_url = str(vocals_path) if vocals_path else str(raw_audio_path) # use vocal if available because it's cleaner for cloning
    
                    if workspace.persist_intermediate:
                        prompt_audio_dir = workspace.ensure_dir(f"prompts/{lang}")
                    else:
                        prompt_audio_dir = workspace.make_temp_dir(f"prompts_{lang}")
                    with step_timer.time(f"prompt_attachment{lang_suffix}"):
                        updated = await run_in_thread(
                            attach_segment_audio_clips,
                            asr_dump=tr_result.model_dump(),
                            output_dir=prompt_audio_dir,
                            min_duration= general_cfg.get("prompt_attachment", {}).get("min_duration", 1.0),
                            max_duration= general_cfg.get("prompt_attachment", {}).get("max_duration", 40.0),
                            one_per_speaker=True,
                        )
                    tr_result_local = ASRResponse(**updated)
    
                    if workspace.persist_intermediate:
                        tts_output_dir = workspace.ensure_dir(f"tts/{lang}")
                    else:
                        tts_output_dir = workspace.make_temp_dir(f"tts_{lang}")
                    with step_timer.time(f"tts{lang_suffix}"):
                        tts_result = await synthesize_tts(client, tts_model_key, tr_result_local, lang, tts_output_dir)
    
    
                    if involve_mode:
                        await run_tts_review_session(
                            run_id=run_id,
                            language=lang,
                            tts_model=tts_model_key,
                            workspace_path=tts_output_dir,
                            translation=tr_result_local,
                            tts_result=tts_result,
                        )
    
                    tr_out_path = workspace.maybe_dump_json(
                        f"translation/{lang}/translation_result.json",
                        tr_result_local.model_dump(),
                    )
    
                    tts_out_path = workspace.maybe_dump_json(
                        f"tts/{lang}/tts_result.json",
                        tts_result.model_dump(),
                    )
    
                    if perform_vad_trimming:
                        if workspace.persist_intermediate:
                            vad_dir = workspace.ensure_dir(f"vad_trimmed/{lang}")
                        else:
                            vad_dir = workspace.make_temp_dir(f"vad_trimmed_{lang}")
                        with step_timer.time(f"tts_vad_trim{lang_suffix}"):
                            tts_result = await trim_tts_segments(tts_result, vad_dir)
                        workspace.maybe_dump_json(
                            f"tts/{lang}/tts_result.json",
                            tts_result.model_dump(),
                        )
    
                    if workspace.persist_intermediate or not source_has_video:
                        audio_processing_dir = workspace.ensure_dir(f"audio_processing/{lang}")
                    else:
                        audio_processing_dir = workspace.make_temp_dir(f"audio_processing_{lang}")
    
    
                    speech_track = audio_processing_dir / f"dubbed_speech_track_{lang}.wav"
                    with step_timer.time(f"audio_concatenate{lang_suffix}"):
                        concatenated_path, translation_segments = await concatenate_segments(
                            tts_result.model_dump()["segments"],
                            speech_track,
                            target_duration=raw_audio_duration,
                            translation_segments=tr_result_local.model_dump()["segments"],
                        )
                    final_audio_path = Path(concatenated_path)
    
                    if dubbing_strategy == "full_replacement" and background_path:
                        with step_timer.time(f"audio_overlay{lang_suffix}"):
                            final_audio_path = audio_processing_dir / f"final_dubbed_audio_{lang}.wav"
                            await overlay_segments_on_background(
                                tts_result.model_dump()["segments"],
                                background_path=background_path,
                                output_path=final_audio_path,
                                sophisticated=sophisticated_dub_timing,
                                speech_track=speech_track,
                            )
                    else:
                        logger.info("Using translation-over dubbing strategy for language %s", lang)
    
                    aligned_srt = ""
                    aligned_vtt = ""
                    if subtitle_style is not None:
                        with step_timer.time(f"dubbed_alignment{lang_suffix}"):
                            aligned_tts = await align_dubbed_audio(
                                client,
                                asr_model,
                                tr_result_local,
                                translation_segments,
                                final_audio_path,
                            )
                        tr_aligned_tts_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_dubbedvoice_result.json",
                            aligned_tts.model_dump(),
                        )
                        if subtitles_dir:
                            aligned_srt, aligned_vtt = build_subtitles_from_asr_result(
                                data=aligned_tts.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                    else:
                        tr_aligned_tts_path = ""
    
                    final_video_out: str = ""
                    if source_has_video:
                        dubbed_path = workspace.file_path(f"dubbed_video_{lang}.mp4")
                        final_output = (
                            workspace.file_path(f"dubbed_video_{lang}_with_{subtitle_style_prefix}_subs.mp4")
                            if subtitle_style is not None
                            else None
                        )
                        with step_timer.time(f"final_pass[{lang}]"):
                            await finalize_media(
                                str(resolved_video_path),
                                final_audio_path,
                                dubbed_path,
                                final_output,
                                Path(aligned_vtt) if aligned_vtt else None,
                                style,
                                subtitle_mobile_mode,
                                dubbing_strategy,
                            )
                        final_video_out = str(final_output) if final_output else str(dubbed_path)
                    else:
                        final_video_out = str(final_audio_path)
    
                    payload = {
                        "final_video_path": final_video_out,
                        "final_audio_path": str(final_audio_path) if final_audio_path else "",
                        "speech_track": str(speech_track),
                        "subtitles": {"aligned": {"srt": aligned_srt, "vtt": aligned_vtt}},
                        "intermediate_files": {
                            "translation": tr_out_path,
                            "translation_aligned_W_origin": tr_aligned_origin_path,
                            "translation_aligned_W_dubbedvoice": tr_aligned_tts_path,
                            "tts": tts_out_path,
                        },
                        "models": {"translation": translation_model_key, "tts": tts_model_key},
                    }
                    return lang, payload
    
                results = await asyncio.gather(*(process_language(lang) for lang in languages_to_process))
                for lang, payload in results:
                    language_payloads[lang] = payload
                    aligned = payload.get("subtitles", {}).get("aligned", {})
                    if aligned:
                        subtitles_per_language[lang] = {"aligned": aligned}
    
                primary_payload = None
                if default_language and default_language in language_payloads:
                    primary_payload = language_payloads[default_language]
                elif language_payloads:
                    default_language = next(iter(language_payloads))
                    primary_payload = language_payloads[default_language]
    
                if primary_payload:
                    final_video_path = primary_payload.get("final_video_path", final_video_path)
                    default_audio_path = primary_payload.get("final_audio_path", "")
                    default_speech_track = primary_payload.get("speech_track", "")
                    aligned = primary_payload.get("subtitles", {}).get("aligned", {})
                    srt_path_1 = aligned.get("srt", srt_path_1)
                    vtt_path_1 = aligned.get("vtt", vtt_path_1)
    
            def keep_if_persistent(path: Optional[str | Path]) -> str:
                if not path:
                    return ""
                return str(path) if (workspace.persist_intermediate or not source_has_video) else ""
    
            default_audio_path = keep_if_persistent(default_audio_path)
            default_speech_track = keep_if_persistent(default_speech_track)
    
            language_outputs_serialized: Dict[str, Dict[str, Any]] = {}
            for lang, payload in language_payloads.items():
                aligned = payload.get("subtitles", {}).get("aligned", {})
                intermediate_serialized = {
                    key: keep_if_persistent(value)
                    for key, value in (payload.get("intermediate_files") or {}).items()
                }
                language_outputs_serialized[lang] = {
                    "final_video_path": payload.get("final_video_path", ""),
                    "final_audio_path": keep_if_persistent(payload.get("final_audio_path")),
                    "speech_track": keep_if_persistent(payload.get("speech_track")),
                    "subtitles": {
                        "aligned": {
                            "srt": keep_if_persistent(aligned.get("srt")),
                            "vtt": keep_if_persistent(aligned.get("vtt")),
                        }
                    },
                    "intermediate_files": intermediate_serialized,
                    "models": payload.get("models", per_language_models.get(lang, {})),
                }
    
            for lang, subs in subtitles_per_language.items():
                aligned = subs.get("aligned", {})
                existing = language_outputs_serialized.setdefault(
                    lang,
                    {
                        "final_video_path": final_video_path if default_language == lang else "",
                        "final_audio_path": "",
                        "speech_track": "",
                        "subtitles": {},
                        "intermediate_files": {},
                        "models": per_language_models.get(lang, {}),
                    },
                )
                existing.setdefault("subtitles", {})
                existing["subtitles"]["aligned"] = {
                    "srt": keep_if_persistent(aligned.get("srt")),
                    "vtt": keep_if_persistent(aligned.get("vtt")),
                }
    
            subtitles_per_language_payload = {
                lang: data.get("subtitles", {})
                for lang, data in language_outputs_serialized.items()
                if data.get("subtitles")
            }
    
            if per_language_models:
                selected_models["per_language"] = per_language_models
    
            default_intermediate = (language_outputs_serialized.get(default_language) or {}).get("intermediate_files", {})
    
            intermediate_files_payload: Dict[str, Any] = {
                "asr_original": keep_if_persistent(asr_raw_path),
                "asr_aligned": keep_if_persistent(asr_aligned_path),
                "translation": default_intermediate.get("translation", ""),
                "translation_aligned_W_origin": default_intermediate.get("translation_aligned_W_origin", ""),
                "translation_aligned_W_dubbedvoice": default_intermediate.get("translation_aligned_W_dubbedvoice", ""),
                "tts": default_intermediate.get("tts", ""),
                "vocals": keep_if_persistent(vocals_path),
                "background": keep_if_persistent(background_path),
                "per_language": {
                    lang: data["intermediate_files"] for lang, data in language_outputs_serialized.items()
                },
            }
    
            subtitles_payload: Dict[str, Any] = {
                "original": {"srt": keep_if_persistent(srt_path_0), "vtt": keep_if_persistent(vtt_path_0)},
                "aligned": {"srt": keep_if_persistent(srt_path_1), "vtt": keep_if_persistent(vtt_path_1)},
            }
            if subtitles_per_language_payload:
                subtitles_payload["per_language"] = {
                    lang: {"aligned": subs.get("aligned", {})}
                    for lang, subs in subtitles_per_language_payload.items()
                    if subs.get("aligned")
                }
    
            final_result: Dict[str, Any] = {
                "workspace_id": workspace.workspace_id,
                "final_video_path": final_video_path,
                "final_audio_path": default_audio_path,
                "speech_track": default_speech_track,
                "source_media": original_source,
                "source_video": keep_if_persistent(source_media_local_path),
                "source_media_local_path": str(source_media_local_path) if source_media_local_path else "",
                "default_language": default_language,
                "available_languages": list(language_outputs_serialized.keys()),
                "language_outputs": language_outputs_serialized,
                "models": selected_models,
                "subtitles": subtitles_payload,
                "intermediate_files": intermediate_files_payload,
                "timings": step_timer.timings,
            }
    
            workspace.maybe_dump_json("final_result.json", final_result)
    
            # FIX: Copy to persistent storage if on Modal
            try:
                await copy_to_persistent_storage(workspace.workspace_id, workspace.workspace)
            except Exception as exc:
                logger.error(f"Failed to persist outputs: {exc}", exc_info=True)
    
            return final_result
    
        except asyncio.CancelledError:
            cancelled = True
            raise
        except HTTPException:
            raise
        except Exception as exc:  # noqa: BLE001
            logger.exception("Pipeline failed: %s", exc)
>           raise HTTPException(500, f"Pipeline failed: {exc}") from exc
E           fastapi.exceptions.HTTPException: 500: Pipeline failed: name 'tr_provider' is not defined

app/main.py:2499: HTTPException
------------------------------ Captured log call -------------------------------
ERROR    bluez.orchestrator:main.py:2498 Pipeline failed: name 'tr_provider' is not defined
Traceback (most recent call last):
  File "/home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator/app/main.py", line 2359, in dub
    results = await asyncio.gather(*(process_language(lang) for lang in languages_to_process))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator/app/main.py", line 2184, in process_language
    tr_provider,  # ADD: provider
    ^^^^^^^^^^^
NameError: name 'tr_provider' is not defined
_____________________ test_dub_pipeline_multiple_languages _____________________

video_url = '/tmp/pytest-of-runner/pytest-3/test_dub_pipeline_multiple_lan0/input.wav'
target_work = 'dub', target_langs = ['fr', 'es'], source_lang = 'en'
min_speakers = None, max_speakers = None
sep_model = 'melband_roformer_big_beta5e.ckpt', asr_model = 'whisperx'
tr_model = 'facebook_m2m100', tts_model = 'chatterbox', audio_sep = False
perform_vad_trimming = False, translation_strategy = 'default'
dubbing_strategy = 'default', sophisticated_dub_timing = True
subtitle_style = None, persist_intermediate = False, involve_mode = False
run_id = None

    @app.post("/v1/dub")
    async def dub(
        video_url: str,
        target_work: str = Query(
            "dub",
            description="Target work type, e.g., 'dub': for full dubbing or 'sub': for subtitles only",
        ),
        target_langs: Optional[List[str] | str] = Query(None),
        source_lang: Optional[str] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        sep_model: str = Query("melband_roformer_big_beta5e.ckpt"),
        asr_model: str = Query("whisperx"),
        tr_model: str = Query("facebook_m2m100"),
        tts_model: str = Query("chatterbox"),
        audio_sep: bool = Query(True, description="Whether to perform audio source separation"),
        perform_vad_trimming: bool = Query(True, description="Whether to perform VAD-based silence trimming after TTS"),
        translation_strategy: str = Query(
            "default",
            description="Translation strategy to use: either translate directly over the short ASR aligned segments or translate the full text and then align the translated result after",
        ),
        dubbing_strategy: str = Query(
            "default",
            description="Dubbing strategy to use, either translation over (original audio ducked) or full replacement",
        ),
        sophisticated_dub_timing: bool = Query(
            True,
            description="Whether to use sophisticated timing for full replacement dubbing strategy",
        ),
        subtitle_style: Optional[str] = Query(
            None,
            description="Subtitle style preset: default, minimal, bold, netflix",
        ),
        persist_intermediate: bool = Query(
            True,
            description="Persist intermediate artifacts (disable for lower latency and disk usage)",
        ),
        involve_mode: bool = Query(
            False,
            description="Enable involve-mode workflow with manual transcription review between stages.",
        ),
        run_id: Optional[str] = Query(
            None,
            description="Optional run identifier when invoked from the job runner (required for involve mode).",
        ),
    ):
        """
        Complete dubbing pipeline orchestrator.
        """
    
        if involve_mode and not run_id:
            raise HTTPException(400, "Involve mode requires an active run context (run_id).")
    
        original_source = video_url
        video_url = video_url.strip()
        if target_work == "sub" and subtitle_style is None:
            subtitle_style = "default_mobile"
        if dubbing_strategy != "translation_over":
            dubbing_strategy = "full_replacement" # other values default to full_replacement
    
        source_lang = (source_lang or "").strip() or None
        target_languages = normalize_language_codes((list(target_langs)) if target_langs else [])
    
        sep_model = (sep_model or "").strip()
    
        requested_tr_model = (tr_model or "").strip()
        requested_tts_model = (tts_model or "").strip()
    
        asr_model = resolve_model_choice(asr_model, ASR_WORKERS, source_lang, fallback= general_cfg.get("default_models", {}).get("asr", "whisperx"))
    
        translation_models_by_lang: Dict[str, str] = {}
        tts_models_by_lang: Dict[str, str] = {}
        per_language_models: Dict[str, Dict[str, str]] = {}
        for lang in target_languages:
            translation_models_by_lang[lang] = resolve_model_choice(
                requested_tr_model,
                TR_WORKERS,
                lang or source_lang,
                fallback= general_cfg.get("default_models", {}).get("tr", "deep_translator"),
            )
            tts_models_by_lang[lang] = resolve_model_choice(
                requested_tts_model,
                TTS_WORKERS,
                lang,
                fallback= general_cfg.get("default_models", {}).get("tts", "chatterbox"),
            )
    
        selected_models = {
            "asr": asr_model,
            "translation": translation_models_by_lang,
            "tts": tts_models_by_lang,
            "separation": sep_model,
        }
    
        if translation_strategy not in TRANSLATION_STRATEGIES:
            translation_strategy = TRANSLATION_STRATEGIES[0]
        if dubbing_strategy not in DUBBING_STRATEGIES:
            dubbing_strategy = DUBBING_STRATEGIES[0]
    
        workspace = WorkspaceManager.create(OUTS, persist_intermediate)
        step_timer = StepTimer()
        client = get_http_client()
    
        subtitles_dir: Optional[Path] = None
    
        resolved_video_path, media_digest = await prepare_media_source(video_url, workspace)
        source_media_local_path: Optional[Path] = resolved_video_path
    
        source_has_video = await has_video_stream(resolved_video_path) # better check to avoid issues with audio-only inputs being treated as videos
    
        if not source_has_video:
            target_work = "dub"
            subtitle_style = None
            dubbing_strategy = "full_replacement"
    
        if workspace.persist_intermediate:
            preprocessing_dir = workspace.ensure_dir("preprocessing")
        else:
            preprocessing_dir = workspace.make_temp_dir("preprocessing")
    
        raw_audio_path = preprocessing_dir / "raw_audio.wav"
    
        cancelled = False
    
        try:
            raw_audio_cached = False
            raw_audio_cache_token: Optional[str] = None
            if media_digest:
                raw_audio_cache_token = raw_audio_cache_key(media_digest)
                raw_audio_cached = await load_cached_raw_audio(raw_audio_cache_token, raw_audio_path)
                if raw_audio_cached:
                    logger.info("Loaded raw audio from cache for media digest %s", media_digest)
    
            if not raw_audio_cached:
                with step_timer.time("extract_audio"):
                    await extract_audio_to_workspace(str(resolved_video_path), raw_audio_path)
                if raw_audio_cache_token:
                    await store_raw_audio_cache(raw_audio_cache_token, raw_audio_path)
    
            raw_audio_duration = get_audio_duration(raw_audio_path)
    
            vocals_path: Optional[Path] = None
            background_path: Optional[Path] = None
    
            vocal_for_transcript = general_cfg.get("vocal_only_for_transcription", True)
            logger.info("Vocal only for transcription: %s", vocal_for_transcript)
    
            if target_work != "sub" or vocal_for_transcript: # in subtitle-only mode, no need to separate audio for now: in future we might want to do it for better ASR performance if we succeed to implement automatic noise level detection
                with step_timer.time("audio_separation"):
    
                    vocals_path, background_path, dubbing_strategy = await maybe_run_audio_separation(
                        preprocessing_dir,
                        raw_audio_path,
                        sep_model,
                        audio_sep,
                        dubbing_strategy,
                    )
    
            transcript_audio = vocals_path if vocals_path and vocal_for_transcript else raw_audio_path
    
            with step_timer.time("asr"):
                raw_asr_result, aligned_asr_result = await run_asr_step(
                    client,
                    transcript_audio,
                    asr_model,
                    source_lang,
                    min_speakers,
                    max_speakers,
                    perform_alignment=not involve_mode,
                )
    
            original_raw_dump = raw_asr_result.model_dump()
            asr_raw_path = ""
    
            if involve_mode:
                languages_set: set[str] = set()
                for worker in ASR_WORKERS.values():
                    worker_langs = getattr(worker, "languages", None)
                    if worker_langs:
                        for lang in worker_langs:
                            normalized = (lang or "").strip().lower()
                            if normalized:
                                languages_set.add(normalized)
                for segment in raw_asr_result.segments:
                    normalized = (segment.lang or "").strip().lower()
                    if normalized:
                        languages_set.add(normalized)
                available_languages = sorted(languages_set)
                TRANSCRIPTION_REVIEW_SESSIONS[run_id] = TranscriptionReviewSession(
                    run_id=run_id,
                    audio_duration=raw_audio_duration,
                    audio_path=raw_asr_result.audio_url or str(raw_audio_path),
                    languages=available_languages,
                    tolerance=TRANSCRIPTION_SEGMENT_TOLERANCE,
                )
                emit_progress({
                    "type": "transcription_review",
                    "run_id": run_id,
                    "raw": original_raw_dump,
                    "languages": available_languages,
                    "duration": raw_audio_duration,
                    "tolerance": TRANSCRIPTION_SEGMENT_TOLERANCE,
                    "artifacts": {"raw_path": asr_raw_path},
                })
                emit_progress({"type": "status", "event": "awaiting_transcription_review"})
                reviewed_result = await wait_for_transcription_review(run_id)
    
                merged_extra = dict(raw_asr_result.extra or {})
                merged_extra.update(reviewed_result.extra or {})
                reviewed_result.extra = merged_extra
                if not reviewed_result.audio_url:
                    reviewed_result.audio_url = raw_asr_result.audio_url or str(raw_audio_path)
                if not reviewed_result.language:
                    reviewed_result.language = raw_asr_result.language
    
                reviewed_result.segments = sorted(
                    reviewed_result.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
    
                raw_asr_result = reviewed_result
                raw_asr_result.extra.setdefault("enable_diarization", True)
                aligned_asr_result = await align_asr_transcription(
                    client,
                    asr_model,
                    raw_asr_result,
                    diarize=True,
                )
                emit_progress({"type": "transcription_review_complete", "run_id": run_id})
    
            asr_raw_path = workspace.maybe_dump_json("asr/asr_0_result.json", raw_asr_result.model_dump())
    
            source_lang = raw_asr_result.language or source_lang
    
    
            if aligned_asr_result is None:
                raise HTTPException(500, "ASR alignment result missing after transcription stage.")
    
            initial_aligned_dump = aligned_asr_result.model_dump()
            asr_aligned_path = ""
    
            if involve_mode:
                speakers = sorted({seg.speaker_id for seg in aligned_asr_result.segments if seg.speaker_id})
                emit_progress(
                    {
                        "type": "alignment_review",
                        "run_id": run_id,
                        "aligned": initial_aligned_dump,
                        "speakers": speakers,
                        "artifacts": {
                            "aligned_path": asr_aligned_path,
                            "raw_path": asr_raw_path,
                        },
                    }
                )
                emit_progress({"type": "status", "event": "awaiting_alignment_review"})
                reviewed_alignment = await wait_for_alignment_review(run_id)
    
                merged_extra = dict(aligned_asr_result.extra or {})
                merged_extra.update(reviewed_alignment.extra or {})
                reviewed_alignment.extra = merged_extra
                if not reviewed_alignment.audio_url:
                    reviewed_alignment.audio_url = aligned_asr_result.audio_url or raw_asr_result.audio_url
                if not reviewed_alignment.language:
                    reviewed_alignment.language = aligned_asr_result.language or raw_asr_result.language
    
                reviewed_alignment.segments = sorted(
                    reviewed_alignment.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
                aligned_asr_result = reviewed_alignment
                emit_progress({"type": "alignment_review_complete", "run_id": run_id})
    
            asr_aligned_path = workspace.maybe_dump_json(
                "asr/asr_0_aligned_result.json",
                aligned_asr_result.model_dump(),
            )
    
            srt_path_0 = ""
            vtt_path_0 = ""
            subtitles_dir: Optional[Path] = None
    
            if subtitle_style is not None:
                with step_timer.time("subtitles_original"):
    
                    if workspace.persist_intermediate:
                        subtitles_dir = workspace.ensure_dir("subtitles")
                    else:
                        subtitles_dir = workspace.make_temp_dir("subtitles")
    
                    srt_path_0, vtt_path_0 = build_subtitles_from_asr_result(
                        data=aligned_asr_result.model_dump(),
                        output_dir=subtitles_dir,
                        custom_name="original",
                        formats=["srt", "vtt"],
                        mobile_mode=subtitle_style.split("_")[-1] == "mobile",
                    )
    
            translation_mode = translation_strategy.split("_")[0]
            segments_for_translation = (
                raw_asr_result.model_dump()["segments"]
                if translation_mode == "long"
                else aligned_asr_result.model_dump()["segments"]
            )
    
            languages_to_process = target_languages
            default_language = target_languages[0] if target_languages else (source_lang if target_work == "sub" else None)
    
            subtitles_per_language: Dict[str, Dict[str, Dict[str, str]]] = {}
            language_payloads: Dict[str, Dict[str, Any]] = {}
    
            srt_path_1 = srt_path_0
            vtt_path_1 = vtt_path_0
            final_video_path = str(resolved_video_path) if source_has_video else ""
            default_audio_path = ""
            default_speech_track = ""
    
            subtitle_style_prefix = subtitle_style.split("_")[0] if subtitle_style is not None else ""
            subtitle_mobile_mode = subtitle_style.split("_")[-1] == "mobile" if subtitle_style is not None else False
            style = STYLE_PRESETS.get(subtitle_style_prefix, STYLE_PRESETS["default"]) if subtitle_style is not None else None
    
            if target_work == "sub":
                # subtitles-only
                if languages_to_process:
                    for lang in languages_to_process:
                        with step_timer.time(f"translation[{lang}]"):
                            tr_result = await translate_segments(
                                client,
                                translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100")),
                                tr_provider,  # ADD: provider
                                segments_for_translation,
                                source_lang,
                                lang,
                            )
                        workspace.maybe_dump_json(
                            f"translation/{lang}/translation_result.json",
                            tr_result.model_dump(),
                        )
    
                        if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                            with step_timer.time(f"translation_alignment[{lang}]"):
                                tr_result = await align_translation_segments(
                                    tr_result,
                                    raw_asr_result,
                                    aligned_asr_result,
                                    translation_strategy,
                                    lang,
                                )
                            workspace.maybe_dump_json(
                                f"translation/{lang}/translation_aligned_W_origin_result.json",
                                tr_result.model_dump(),
                            )
    
                        if subtitle_style is not None and subtitles_dir:
                            trans_srt, trans_vtt = build_subtitles_from_asr_result(
                                data=tr_result.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                            subtitles_per_language[lang] = {"aligned": {"srt": trans_srt, "vtt": trans_vtt}}
    
                    if default_language and default_language in subtitles_per_language:
                        align = subtitles_per_language[default_language]["aligned"]
                        srt_path_1 = align.get("srt", srt_path_1)
                        vtt_path_1 = align.get("vtt", vtt_path_1)
    
                dubbed_path = resolved_video_path
                final_output = (
                    workspace.file_path(f"subtitled_video_{default_language or source_lang}_with_{subtitle_style_prefix}_subs.mp4")
                    if subtitle_style is not None
                    else None
                )
    
                with step_timer.time("final_pass"):
                    await finalize_media(
                        str(resolved_video_path),
                        None,
                        dubbed_path,
                        final_output,
                        Path(vtt_path_1) if vtt_path_1 else None,
                        style,
                        subtitle_mobile_mode,
                        dubbing_strategy,
                    )
    
                final_video_path = str(final_output) if final_output else str(dubbed_path)
            else:
                # dubbing
                if not languages_to_process:
                    raise HTTPException(400, "At least one target language must be specified for dubbing")
    
                async def process_language(lang: str) -> Tuple[str, Dict[str, Any]]:
                    lang_suffix = f"[{lang}]"
                    translation_model_key = translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100"))
                    tts_model_key = tts_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tts", "chatterbox"))
                    per_language_models[lang] = {"translation": translation_model_key, "tts": tts_model_key}
    
                    with step_timer.time(f"translation{lang_suffix}"):
                        tr_result = await translate_segments(
                            client,
                            translation_model_key,
                            tr_provider,  # ADD: provider
                            segments_for_translation,
                            source_lang,
                            lang,
                        )
    
    
                    if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                        with step_timer.time(f"translation_alignment{lang_suffix}"):
                            tr_result = await align_translation_segments(
                                tr_result,
                                raw_asr_result,
                                aligned_asr_result,
                                translation_strategy,
                                lang,
                            )
                        tr_aligned_origin_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_origin_result.json",
                            tr_result.model_dump(),
                        )
                    else:
                        tr_aligned_origin_path = ""
    
                    ensure_segment_ids(tr_result)
    
                    tr_result.audio_url = str(vocals_path) if vocals_path else str(raw_audio_path) # use vocal if available because it's cleaner for cloning
    
                    if workspace.persist_intermediate:
                        prompt_audio_dir = workspace.ensure_dir(f"prompts/{lang}")
                    else:
                        prompt_audio_dir = workspace.make_temp_dir(f"prompts_{lang}")
                    with step_timer.time(f"prompt_attachment{lang_suffix}"):
                        updated = await run_in_thread(
                            attach_segment_audio_clips,
                            asr_dump=tr_result.model_dump(),
                            output_dir=prompt_audio_dir,
                            min_duration= general_cfg.get("prompt_attachment", {}).get("min_duration", 1.0),
                            max_duration= general_cfg.get("prompt_attachment", {}).get("max_duration", 40.0),
                            one_per_speaker=True,
                        )
                    tr_result_local = ASRResponse(**updated)
    
                    if workspace.persist_intermediate:
                        tts_output_dir = workspace.ensure_dir(f"tts/{lang}")
                    else:
                        tts_output_dir = workspace.make_temp_dir(f"tts_{lang}")
                    with step_timer.time(f"tts{lang_suffix}"):
                        tts_result = await synthesize_tts(client, tts_model_key, tr_result_local, lang, tts_output_dir)
    
    
                    if involve_mode:
                        await run_tts_review_session(
                            run_id=run_id,
                            language=lang,
                            tts_model=tts_model_key,
                            workspace_path=tts_output_dir,
                            translation=tr_result_local,
                            tts_result=tts_result,
                        )
    
                    tr_out_path = workspace.maybe_dump_json(
                        f"translation/{lang}/translation_result.json",
                        tr_result_local.model_dump(),
                    )
    
                    tts_out_path = workspace.maybe_dump_json(
                        f"tts/{lang}/tts_result.json",
                        tts_result.model_dump(),
                    )
    
                    if perform_vad_trimming:
                        if workspace.persist_intermediate:
                            vad_dir = workspace.ensure_dir(f"vad_trimmed/{lang}")
                        else:
                            vad_dir = workspace.make_temp_dir(f"vad_trimmed_{lang}")
                        with step_timer.time(f"tts_vad_trim{lang_suffix}"):
                            tts_result = await trim_tts_segments(tts_result, vad_dir)
                        workspace.maybe_dump_json(
                            f"tts/{lang}/tts_result.json",
                            tts_result.model_dump(),
                        )
    
                    if workspace.persist_intermediate or not source_has_video:
                        audio_processing_dir = workspace.ensure_dir(f"audio_processing/{lang}")
                    else:
                        audio_processing_dir = workspace.make_temp_dir(f"audio_processing_{lang}")
    
    
                    speech_track = audio_processing_dir / f"dubbed_speech_track_{lang}.wav"
                    with step_timer.time(f"audio_concatenate{lang_suffix}"):
                        concatenated_path, translation_segments = await concatenate_segments(
                            tts_result.model_dump()["segments"],
                            speech_track,
                            target_duration=raw_audio_duration,
                            translation_segments=tr_result_local.model_dump()["segments"],
                        )
                    final_audio_path = Path(concatenated_path)
    
                    if dubbing_strategy == "full_replacement" and background_path:
                        with step_timer.time(f"audio_overlay{lang_suffix}"):
                            final_audio_path = audio_processing_dir / f"final_dubbed_audio_{lang}.wav"
                            await overlay_segments_on_background(
                                tts_result.model_dump()["segments"],
                                background_path=background_path,
                                output_path=final_audio_path,
                                sophisticated=sophisticated_dub_timing,
                                speech_track=speech_track,
                            )
                    else:
                        logger.info("Using translation-over dubbing strategy for language %s", lang)
    
                    aligned_srt = ""
                    aligned_vtt = ""
                    if subtitle_style is not None:
                        with step_timer.time(f"dubbed_alignment{lang_suffix}"):
                            aligned_tts = await align_dubbed_audio(
                                client,
                                asr_model,
                                tr_result_local,
                                translation_segments,
                                final_audio_path,
                            )
                        tr_aligned_tts_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_dubbedvoice_result.json",
                            aligned_tts.model_dump(),
                        )
                        if subtitles_dir:
                            aligned_srt, aligned_vtt = build_subtitles_from_asr_result(
                                data=aligned_tts.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                    else:
                        tr_aligned_tts_path = ""
    
                    final_video_out: str = ""
                    if source_has_video:
                        dubbed_path = workspace.file_path(f"dubbed_video_{lang}.mp4")
                        final_output = (
                            workspace.file_path(f"dubbed_video_{lang}_with_{subtitle_style_prefix}_subs.mp4")
                            if subtitle_style is not None
                            else None
                        )
                        with step_timer.time(f"final_pass[{lang}]"):
                            await finalize_media(
                                str(resolved_video_path),
                                final_audio_path,
                                dubbed_path,
                                final_output,
                                Path(aligned_vtt) if aligned_vtt else None,
                                style,
                                subtitle_mobile_mode,
                                dubbing_strategy,
                            )
                        final_video_out = str(final_output) if final_output else str(dubbed_path)
                    else:
                        final_video_out = str(final_audio_path)
    
                    payload = {
                        "final_video_path": final_video_out,
                        "final_audio_path": str(final_audio_path) if final_audio_path else "",
                        "speech_track": str(speech_track),
                        "subtitles": {"aligned": {"srt": aligned_srt, "vtt": aligned_vtt}},
                        "intermediate_files": {
                            "translation": tr_out_path,
                            "translation_aligned_W_origin": tr_aligned_origin_path,
                            "translation_aligned_W_dubbedvoice": tr_aligned_tts_path,
                            "tts": tts_out_path,
                        },
                        "models": {"translation": translation_model_key, "tts": tts_model_key},
                    }
                    return lang, payload
    
>               results = await asyncio.gather(*(process_language(lang) for lang in languages_to_process))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

app/main.py:2359: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

lang = 'fr'

    async def process_language(lang: str) -> Tuple[str, Dict[str, Any]]:
        lang_suffix = f"[{lang}]"
        translation_model_key = translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100"))
        tts_model_key = tts_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tts", "chatterbox"))
        per_language_models[lang] = {"translation": translation_model_key, "tts": tts_model_key}
    
        with step_timer.time(f"translation{lang_suffix}"):
            tr_result = await translate_segments(
                client,
                translation_model_key,
>               tr_provider,  # ADD: provider
                ^^^^^^^^^^^
                segments_for_translation,
                source_lang,
                lang,
            )
E           NameError: name 'tr_provider' is not defined

app/main.py:2184: NameError

The above exception was the direct cause of the following exception:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fc1a1978d90>
tmp_path = PosixPath('/tmp/pytest-of-runner/pytest-3/test_dub_pipeline_multiple_lan0')

    @pytest.mark.asyncio
    async def test_dub_pipeline_multiple_languages(monkeypatch, tmp_path):
        input_wav = tmp_path / "input.wav"
        with wave.open(str(input_wav), "w") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(b"\x00\x00" * 16000)
    
        original_outs = orchestrator_main.OUTS
        orchestrator_main.OUTS = tmp_path
    
        async def fake_maybe_run_audio_separation(*args, **kwargs):  # noqa: ANN001
            return None, None, "default"
    
        async def fake_run_asr_step(*args, **kwargs):  # noqa: ANN001
            seg = Segment(start=0.0, end=1.0, text="Hello", speaker_id="spk1", lang="en")
            response = ASRResponse(segments=[seg], language="en")
            return response, response
    
        translations = {"fr": "Bonjour", "es": "Hola"}
    
        async def fake_run_translation_step(*args, **kwargs):  # noqa: ANN001
            target = args[4] if len(args) > 4 else kwargs.get("target_lang", "fr")
            text = translations.get(target, "Hello")
            seg = Segment(start=0.0, end=1.0, text=text, speaker_id="spk1", lang=target)
            return ASRResponse(segments=[seg], language=target)
    
        tts_audio = tmp_path / "tts_multi.wav"
        with wave.open(str(tts_audio), "w") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(b"\x00\x00" * 16000)
    
        async def fake_synthesize_tts(*args, **kwargs):  # noqa: ANN001
            target_lang = args[3] if len(args) > 3 else kwargs.get("target_lang", "fr")
            segment_out = SegmentAudioOut(
                start=0.0,
                end=1.0,
                audio_url=str(tts_audio),
                speaker_id="spk1",
                lang=target_lang,
                sample_rate=16000,
            )
            return TTSResponse(segments=[segment_out])
    
        async def fake_trim_tts_segments(tts_result, vad_dir):  # noqa: ANN001
            return tts_result
    
        async def fake_concatenate_segments(*args, **kwargs):  # noqa: ANN001
            lang = "multi"
            translation_segments = kwargs.get("translation_segments") or []
            if translation_segments:
                lang = translation_segments[0].get("lang", "multi")
            speech_track = tmp_path / f"speech_{lang}.wav"
            speech_track.write_bytes(tts_audio.read_bytes())
            return str(speech_track), translation_segments or [{"start": 0.0, "end": 1.0, "text": translations.get(lang, "Hello"), "speaker_id": "spk1", "lang": lang}]
    
        async def fake_align_dubbed_audio(*args, **kwargs):  # noqa: ANN001
            tr_result = args[2] if len(args) > 2 else kwargs.get("tr_result")
            return tr_result or ASRResponse(segments=[])
    
        async def fake_finalize_media(
            video_path,
            audio_path,
            dubbed_path,
            output_path,
            subtitle_path,
            sub_style,
            mobile_optimized,
            dubbing_strategy,
        ):  # noqa: ANN001
            Path(dubbed_path).write_text("video")
            if output_path:
                Path(output_path).write_text("video with subtitles")
    
        monkeypatch.setattr(orchestrator_main, "maybe_run_audio_separation", fake_maybe_run_audio_separation)
        monkeypatch.setattr(orchestrator_main, "run_asr_step", fake_run_asr_step)
        monkeypatch.setattr(orchestrator_main, "translate_segments", fake_run_translation_step)
        monkeypatch.setattr(orchestrator_main, "synthesize_tts", fake_synthesize_tts)
        monkeypatch.setattr(orchestrator_main, "trim_tts_segments", fake_trim_tts_segments)
        monkeypatch.setattr(orchestrator_main, "concatenate_segments", fake_concatenate_segments)
        monkeypatch.setattr(orchestrator_main, "align_dubbed_audio", fake_align_dubbed_audio)
        monkeypatch.setattr(orchestrator_main, "finalize_media", fake_finalize_media)
        monkeypatch.setattr(orchestrator_main, "get_audio_duration", lambda *args, **kwargs: 1.0)  # noqa: ARG005
    
        await orchestrator_main.startup_event()
        try:
>           result = await orchestrator_main.dub(
                video_url=str(input_wav),
                target_work="dub",
                target_langs=["fr", "es"],
                source_lang="en",
                translation_strategy="default",
                dubbing_strategy="default",
                sophisticated_dub_timing=True,
                subtitle_style="default",
                audio_sep=False,
                perform_vad_trimming=False,
                persist_intermediate=False,
                sep_model="melband_roformer_big_beta5e.ckpt",
                asr_model="whisperx",
                tr_model="facebook_m2m100",
                tts_model="chatterbox",
                run_id=None,
                involve_mode=False,
            )

tests/test_pipeline_integration.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

video_url = '/tmp/pytest-of-runner/pytest-3/test_dub_pipeline_multiple_lan0/input.wav'
target_work = 'dub', target_langs = ['fr', 'es'], source_lang = 'en'
min_speakers = None, max_speakers = None
sep_model = 'melband_roformer_big_beta5e.ckpt', asr_model = 'whisperx'
tr_model = 'facebook_m2m100', tts_model = 'chatterbox', audio_sep = False
perform_vad_trimming = False, translation_strategy = 'default'
dubbing_strategy = 'default', sophisticated_dub_timing = True
subtitle_style = None, persist_intermediate = False, involve_mode = False
run_id = None

    @app.post("/v1/dub")
    async def dub(
        video_url: str,
        target_work: str = Query(
            "dub",
            description="Target work type, e.g., 'dub': for full dubbing or 'sub': for subtitles only",
        ),
        target_langs: Optional[List[str] | str] = Query(None),
        source_lang: Optional[str] = None,
        min_speakers: Optional[int] = None,
        max_speakers: Optional[int] = None,
        sep_model: str = Query("melband_roformer_big_beta5e.ckpt"),
        asr_model: str = Query("whisperx"),
        tr_model: str = Query("facebook_m2m100"),
        tts_model: str = Query("chatterbox"),
        audio_sep: bool = Query(True, description="Whether to perform audio source separation"),
        perform_vad_trimming: bool = Query(True, description="Whether to perform VAD-based silence trimming after TTS"),
        translation_strategy: str = Query(
            "default",
            description="Translation strategy to use: either translate directly over the short ASR aligned segments or translate the full text and then align the translated result after",
        ),
        dubbing_strategy: str = Query(
            "default",
            description="Dubbing strategy to use, either translation over (original audio ducked) or full replacement",
        ),
        sophisticated_dub_timing: bool = Query(
            True,
            description="Whether to use sophisticated timing for full replacement dubbing strategy",
        ),
        subtitle_style: Optional[str] = Query(
            None,
            description="Subtitle style preset: default, minimal, bold, netflix",
        ),
        persist_intermediate: bool = Query(
            True,
            description="Persist intermediate artifacts (disable for lower latency and disk usage)",
        ),
        involve_mode: bool = Query(
            False,
            description="Enable involve-mode workflow with manual transcription review between stages.",
        ),
        run_id: Optional[str] = Query(
            None,
            description="Optional run identifier when invoked from the job runner (required for involve mode).",
        ),
    ):
        """
        Complete dubbing pipeline orchestrator.
        """
    
        if involve_mode and not run_id:
            raise HTTPException(400, "Involve mode requires an active run context (run_id).")
    
        original_source = video_url
        video_url = video_url.strip()
        if target_work == "sub" and subtitle_style is None:
            subtitle_style = "default_mobile"
        if dubbing_strategy != "translation_over":
            dubbing_strategy = "full_replacement" # other values default to full_replacement
    
        source_lang = (source_lang or "").strip() or None
        target_languages = normalize_language_codes((list(target_langs)) if target_langs else [])
    
        sep_model = (sep_model or "").strip()
    
        requested_tr_model = (tr_model or "").strip()
        requested_tts_model = (tts_model or "").strip()
    
        asr_model = resolve_model_choice(asr_model, ASR_WORKERS, source_lang, fallback= general_cfg.get("default_models", {}).get("asr", "whisperx"))
    
        translation_models_by_lang: Dict[str, str] = {}
        tts_models_by_lang: Dict[str, str] = {}
        per_language_models: Dict[str, Dict[str, str]] = {}
        for lang in target_languages:
            translation_models_by_lang[lang] = resolve_model_choice(
                requested_tr_model,
                TR_WORKERS,
                lang or source_lang,
                fallback= general_cfg.get("default_models", {}).get("tr", "deep_translator"),
            )
            tts_models_by_lang[lang] = resolve_model_choice(
                requested_tts_model,
                TTS_WORKERS,
                lang,
                fallback= general_cfg.get("default_models", {}).get("tts", "chatterbox"),
            )
    
        selected_models = {
            "asr": asr_model,
            "translation": translation_models_by_lang,
            "tts": tts_models_by_lang,
            "separation": sep_model,
        }
    
        if translation_strategy not in TRANSLATION_STRATEGIES:
            translation_strategy = TRANSLATION_STRATEGIES[0]
        if dubbing_strategy not in DUBBING_STRATEGIES:
            dubbing_strategy = DUBBING_STRATEGIES[0]
    
        workspace = WorkspaceManager.create(OUTS, persist_intermediate)
        step_timer = StepTimer()
        client = get_http_client()
    
        subtitles_dir: Optional[Path] = None
    
        resolved_video_path, media_digest = await prepare_media_source(video_url, workspace)
        source_media_local_path: Optional[Path] = resolved_video_path
    
        source_has_video = await has_video_stream(resolved_video_path) # better check to avoid issues with audio-only inputs being treated as videos
    
        if not source_has_video:
            target_work = "dub"
            subtitle_style = None
            dubbing_strategy = "full_replacement"
    
        if workspace.persist_intermediate:
            preprocessing_dir = workspace.ensure_dir("preprocessing")
        else:
            preprocessing_dir = workspace.make_temp_dir("preprocessing")
    
        raw_audio_path = preprocessing_dir / "raw_audio.wav"
    
        cancelled = False
    
        try:
            raw_audio_cached = False
            raw_audio_cache_token: Optional[str] = None
            if media_digest:
                raw_audio_cache_token = raw_audio_cache_key(media_digest)
                raw_audio_cached = await load_cached_raw_audio(raw_audio_cache_token, raw_audio_path)
                if raw_audio_cached:
                    logger.info("Loaded raw audio from cache for media digest %s", media_digest)
    
            if not raw_audio_cached:
                with step_timer.time("extract_audio"):
                    await extract_audio_to_workspace(str(resolved_video_path), raw_audio_path)
                if raw_audio_cache_token:
                    await store_raw_audio_cache(raw_audio_cache_token, raw_audio_path)
    
            raw_audio_duration = get_audio_duration(raw_audio_path)
    
            vocals_path: Optional[Path] = None
            background_path: Optional[Path] = None
    
            vocal_for_transcript = general_cfg.get("vocal_only_for_transcription", True)
            logger.info("Vocal only for transcription: %s", vocal_for_transcript)
    
            if target_work != "sub" or vocal_for_transcript: # in subtitle-only mode, no need to separate audio for now: in future we might want to do it for better ASR performance if we succeed to implement automatic noise level detection
                with step_timer.time("audio_separation"):
    
                    vocals_path, background_path, dubbing_strategy = await maybe_run_audio_separation(
                        preprocessing_dir,
                        raw_audio_path,
                        sep_model,
                        audio_sep,
                        dubbing_strategy,
                    )
    
            transcript_audio = vocals_path if vocals_path and vocal_for_transcript else raw_audio_path
    
            with step_timer.time("asr"):
                raw_asr_result, aligned_asr_result = await run_asr_step(
                    client,
                    transcript_audio,
                    asr_model,
                    source_lang,
                    min_speakers,
                    max_speakers,
                    perform_alignment=not involve_mode,
                )
    
            original_raw_dump = raw_asr_result.model_dump()
            asr_raw_path = ""
    
            if involve_mode:
                languages_set: set[str] = set()
                for worker in ASR_WORKERS.values():
                    worker_langs = getattr(worker, "languages", None)
                    if worker_langs:
                        for lang in worker_langs:
                            normalized = (lang or "").strip().lower()
                            if normalized:
                                languages_set.add(normalized)
                for segment in raw_asr_result.segments:
                    normalized = (segment.lang or "").strip().lower()
                    if normalized:
                        languages_set.add(normalized)
                available_languages = sorted(languages_set)
                TRANSCRIPTION_REVIEW_SESSIONS[run_id] = TranscriptionReviewSession(
                    run_id=run_id,
                    audio_duration=raw_audio_duration,
                    audio_path=raw_asr_result.audio_url or str(raw_audio_path),
                    languages=available_languages,
                    tolerance=TRANSCRIPTION_SEGMENT_TOLERANCE,
                )
                emit_progress({
                    "type": "transcription_review",
                    "run_id": run_id,
                    "raw": original_raw_dump,
                    "languages": available_languages,
                    "duration": raw_audio_duration,
                    "tolerance": TRANSCRIPTION_SEGMENT_TOLERANCE,
                    "artifacts": {"raw_path": asr_raw_path},
                })
                emit_progress({"type": "status", "event": "awaiting_transcription_review"})
                reviewed_result = await wait_for_transcription_review(run_id)
    
                merged_extra = dict(raw_asr_result.extra or {})
                merged_extra.update(reviewed_result.extra or {})
                reviewed_result.extra = merged_extra
                if not reviewed_result.audio_url:
                    reviewed_result.audio_url = raw_asr_result.audio_url or str(raw_audio_path)
                if not reviewed_result.language:
                    reviewed_result.language = raw_asr_result.language
    
                reviewed_result.segments = sorted(
                    reviewed_result.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
    
                raw_asr_result = reviewed_result
                raw_asr_result.extra.setdefault("enable_diarization", True)
                aligned_asr_result = await align_asr_transcription(
                    client,
                    asr_model,
                    raw_asr_result,
                    diarize=True,
                )
                emit_progress({"type": "transcription_review_complete", "run_id": run_id})
    
            asr_raw_path = workspace.maybe_dump_json("asr/asr_0_result.json", raw_asr_result.model_dump())
    
            source_lang = raw_asr_result.language or source_lang
    
    
            if aligned_asr_result is None:
                raise HTTPException(500, "ASR alignment result missing after transcription stage.")
    
            initial_aligned_dump = aligned_asr_result.model_dump()
            asr_aligned_path = ""
    
            if involve_mode:
                speakers = sorted({seg.speaker_id for seg in aligned_asr_result.segments if seg.speaker_id})
                emit_progress(
                    {
                        "type": "alignment_review",
                        "run_id": run_id,
                        "aligned": initial_aligned_dump,
                        "speakers": speakers,
                        "artifacts": {
                            "aligned_path": asr_aligned_path,
                            "raw_path": asr_raw_path,
                        },
                    }
                )
                emit_progress({"type": "status", "event": "awaiting_alignment_review"})
                reviewed_alignment = await wait_for_alignment_review(run_id)
    
                merged_extra = dict(aligned_asr_result.extra or {})
                merged_extra.update(reviewed_alignment.extra or {})
                reviewed_alignment.extra = merged_extra
                if not reviewed_alignment.audio_url:
                    reviewed_alignment.audio_url = aligned_asr_result.audio_url or raw_asr_result.audio_url
                if not reviewed_alignment.language:
                    reviewed_alignment.language = aligned_asr_result.language or raw_asr_result.language
    
                reviewed_alignment.segments = sorted(
                    reviewed_alignment.segments,
                    key=lambda seg: seg.start if seg.start is not None else 0.0,
                )
                aligned_asr_result = reviewed_alignment
                emit_progress({"type": "alignment_review_complete", "run_id": run_id})
    
            asr_aligned_path = workspace.maybe_dump_json(
                "asr/asr_0_aligned_result.json",
                aligned_asr_result.model_dump(),
            )
    
            srt_path_0 = ""
            vtt_path_0 = ""
            subtitles_dir: Optional[Path] = None
    
            if subtitle_style is not None:
                with step_timer.time("subtitles_original"):
    
                    if workspace.persist_intermediate:
                        subtitles_dir = workspace.ensure_dir("subtitles")
                    else:
                        subtitles_dir = workspace.make_temp_dir("subtitles")
    
                    srt_path_0, vtt_path_0 = build_subtitles_from_asr_result(
                        data=aligned_asr_result.model_dump(),
                        output_dir=subtitles_dir,
                        custom_name="original",
                        formats=["srt", "vtt"],
                        mobile_mode=subtitle_style.split("_")[-1] == "mobile",
                    )
    
            translation_mode = translation_strategy.split("_")[0]
            segments_for_translation = (
                raw_asr_result.model_dump()["segments"]
                if translation_mode == "long"
                else aligned_asr_result.model_dump()["segments"]
            )
    
            languages_to_process = target_languages
            default_language = target_languages[0] if target_languages else (source_lang if target_work == "sub" else None)
    
            subtitles_per_language: Dict[str, Dict[str, Dict[str, str]]] = {}
            language_payloads: Dict[str, Dict[str, Any]] = {}
    
            srt_path_1 = srt_path_0
            vtt_path_1 = vtt_path_0
            final_video_path = str(resolved_video_path) if source_has_video else ""
            default_audio_path = ""
            default_speech_track = ""
    
            subtitle_style_prefix = subtitle_style.split("_")[0] if subtitle_style is not None else ""
            subtitle_mobile_mode = subtitle_style.split("_")[-1] == "mobile" if subtitle_style is not None else False
            style = STYLE_PRESETS.get(subtitle_style_prefix, STYLE_PRESETS["default"]) if subtitle_style is not None else None
    
            if target_work == "sub":
                # subtitles-only
                if languages_to_process:
                    for lang in languages_to_process:
                        with step_timer.time(f"translation[{lang}]"):
                            tr_result = await translate_segments(
                                client,
                                translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100")),
                                tr_provider,  # ADD: provider
                                segments_for_translation,
                                source_lang,
                                lang,
                            )
                        workspace.maybe_dump_json(
                            f"translation/{lang}/translation_result.json",
                            tr_result.model_dump(),
                        )
    
                        if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                            with step_timer.time(f"translation_alignment[{lang}]"):
                                tr_result = await align_translation_segments(
                                    tr_result,
                                    raw_asr_result,
                                    aligned_asr_result,
                                    translation_strategy,
                                    lang,
                                )
                            workspace.maybe_dump_json(
                                f"translation/{lang}/translation_aligned_W_origin_result.json",
                                tr_result.model_dump(),
                            )
    
                        if subtitle_style is not None and subtitles_dir:
                            trans_srt, trans_vtt = build_subtitles_from_asr_result(
                                data=tr_result.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                            subtitles_per_language[lang] = {"aligned": {"srt": trans_srt, "vtt": trans_vtt}}
    
                    if default_language and default_language in subtitles_per_language:
                        align = subtitles_per_language[default_language]["aligned"]
                        srt_path_1 = align.get("srt", srt_path_1)
                        vtt_path_1 = align.get("vtt", vtt_path_1)
    
                dubbed_path = resolved_video_path
                final_output = (
                    workspace.file_path(f"subtitled_video_{default_language or source_lang}_with_{subtitle_style_prefix}_subs.mp4")
                    if subtitle_style is not None
                    else None
                )
    
                with step_timer.time("final_pass"):
                    await finalize_media(
                        str(resolved_video_path),
                        None,
                        dubbed_path,
                        final_output,
                        Path(vtt_path_1) if vtt_path_1 else None,
                        style,
                        subtitle_mobile_mode,
                        dubbing_strategy,
                    )
    
                final_video_path = str(final_output) if final_output else str(dubbed_path)
            else:
                # dubbing
                if not languages_to_process:
                    raise HTTPException(400, "At least one target language must be specified for dubbing")
    
                async def process_language(lang: str) -> Tuple[str, Dict[str, Any]]:
                    lang_suffix = f"[{lang}]"
                    translation_model_key = translation_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tr", "facebook_m2m100"))
                    tts_model_key = tts_models_by_lang.get(lang, general_cfg.get("default_models", {}).get("tts", "chatterbox"))
                    per_language_models[lang] = {"translation": translation_model_key, "tts": tts_model_key}
    
                    with step_timer.time(f"translation{lang_suffix}"):
                        tr_result = await translate_segments(
                            client,
                            translation_model_key,
                            tr_provider,  # ADD: provider
                            segments_for_translation,
                            source_lang,
                            lang,
                        )
    
    
                    if translation_mode == "long" and len(aligned_asr_result.segments) > 1:
                        with step_timer.time(f"translation_alignment{lang_suffix}"):
                            tr_result = await align_translation_segments(
                                tr_result,
                                raw_asr_result,
                                aligned_asr_result,
                                translation_strategy,
                                lang,
                            )
                        tr_aligned_origin_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_origin_result.json",
                            tr_result.model_dump(),
                        )
                    else:
                        tr_aligned_origin_path = ""
    
                    ensure_segment_ids(tr_result)
    
                    tr_result.audio_url = str(vocals_path) if vocals_path else str(raw_audio_path) # use vocal if available because it's cleaner for cloning
    
                    if workspace.persist_intermediate:
                        prompt_audio_dir = workspace.ensure_dir(f"prompts/{lang}")
                    else:
                        prompt_audio_dir = workspace.make_temp_dir(f"prompts_{lang}")
                    with step_timer.time(f"prompt_attachment{lang_suffix}"):
                        updated = await run_in_thread(
                            attach_segment_audio_clips,
                            asr_dump=tr_result.model_dump(),
                            output_dir=prompt_audio_dir,
                            min_duration= general_cfg.get("prompt_attachment", {}).get("min_duration", 1.0),
                            max_duration= general_cfg.get("prompt_attachment", {}).get("max_duration", 40.0),
                            one_per_speaker=True,
                        )
                    tr_result_local = ASRResponse(**updated)
    
                    if workspace.persist_intermediate:
                        tts_output_dir = workspace.ensure_dir(f"tts/{lang}")
                    else:
                        tts_output_dir = workspace.make_temp_dir(f"tts_{lang}")
                    with step_timer.time(f"tts{lang_suffix}"):
                        tts_result = await synthesize_tts(client, tts_model_key, tr_result_local, lang, tts_output_dir)
    
    
                    if involve_mode:
                        await run_tts_review_session(
                            run_id=run_id,
                            language=lang,
                            tts_model=tts_model_key,
                            workspace_path=tts_output_dir,
                            translation=tr_result_local,
                            tts_result=tts_result,
                        )
    
                    tr_out_path = workspace.maybe_dump_json(
                        f"translation/{lang}/translation_result.json",
                        tr_result_local.model_dump(),
                    )
    
                    tts_out_path = workspace.maybe_dump_json(
                        f"tts/{lang}/tts_result.json",
                        tts_result.model_dump(),
                    )
    
                    if perform_vad_trimming:
                        if workspace.persist_intermediate:
                            vad_dir = workspace.ensure_dir(f"vad_trimmed/{lang}")
                        else:
                            vad_dir = workspace.make_temp_dir(f"vad_trimmed_{lang}")
                        with step_timer.time(f"tts_vad_trim{lang_suffix}"):
                            tts_result = await trim_tts_segments(tts_result, vad_dir)
                        workspace.maybe_dump_json(
                            f"tts/{lang}/tts_result.json",
                            tts_result.model_dump(),
                        )
    
                    if workspace.persist_intermediate or not source_has_video:
                        audio_processing_dir = workspace.ensure_dir(f"audio_processing/{lang}")
                    else:
                        audio_processing_dir = workspace.make_temp_dir(f"audio_processing_{lang}")
    
    
                    speech_track = audio_processing_dir / f"dubbed_speech_track_{lang}.wav"
                    with step_timer.time(f"audio_concatenate{lang_suffix}"):
                        concatenated_path, translation_segments = await concatenate_segments(
                            tts_result.model_dump()["segments"],
                            speech_track,
                            target_duration=raw_audio_duration,
                            translation_segments=tr_result_local.model_dump()["segments"],
                        )
                    final_audio_path = Path(concatenated_path)
    
                    if dubbing_strategy == "full_replacement" and background_path:
                        with step_timer.time(f"audio_overlay{lang_suffix}"):
                            final_audio_path = audio_processing_dir / f"final_dubbed_audio_{lang}.wav"
                            await overlay_segments_on_background(
                                tts_result.model_dump()["segments"],
                                background_path=background_path,
                                output_path=final_audio_path,
                                sophisticated=sophisticated_dub_timing,
                                speech_track=speech_track,
                            )
                    else:
                        logger.info("Using translation-over dubbing strategy for language %s", lang)
    
                    aligned_srt = ""
                    aligned_vtt = ""
                    if subtitle_style is not None:
                        with step_timer.time(f"dubbed_alignment{lang_suffix}"):
                            aligned_tts = await align_dubbed_audio(
                                client,
                                asr_model,
                                tr_result_local,
                                translation_segments,
                                final_audio_path,
                            )
                        tr_aligned_tts_path = workspace.maybe_dump_json(
                            f"translation/{lang}/translation_aligned_W_dubbedvoice_result.json",
                            aligned_tts.model_dump(),
                        )
                        if subtitles_dir:
                            aligned_srt, aligned_vtt = build_subtitles_from_asr_result(
                                data=aligned_tts.model_dump(),
                                output_dir=subtitles_dir,
                                custom_name=f"dubbed_{lang}",
                                formats=["srt", "vtt"],
                                mobile_mode=subtitle_mobile_mode,
                            )
                    else:
                        tr_aligned_tts_path = ""
    
                    final_video_out: str = ""
                    if source_has_video:
                        dubbed_path = workspace.file_path(f"dubbed_video_{lang}.mp4")
                        final_output = (
                            workspace.file_path(f"dubbed_video_{lang}_with_{subtitle_style_prefix}_subs.mp4")
                            if subtitle_style is not None
                            else None
                        )
                        with step_timer.time(f"final_pass[{lang}]"):
                            await finalize_media(
                                str(resolved_video_path),
                                final_audio_path,
                                dubbed_path,
                                final_output,
                                Path(aligned_vtt) if aligned_vtt else None,
                                style,
                                subtitle_mobile_mode,
                                dubbing_strategy,
                            )
                        final_video_out = str(final_output) if final_output else str(dubbed_path)
                    else:
                        final_video_out = str(final_audio_path)
    
                    payload = {
                        "final_video_path": final_video_out,
                        "final_audio_path": str(final_audio_path) if final_audio_path else "",
                        "speech_track": str(speech_track),
                        "subtitles": {"aligned": {"srt": aligned_srt, "vtt": aligned_vtt}},
                        "intermediate_files": {
                            "translation": tr_out_path,
                            "translation_aligned_W_origin": tr_aligned_origin_path,
                            "translation_aligned_W_dubbedvoice": tr_aligned_tts_path,
                            "tts": tts_out_path,
                        },
                        "models": {"translation": translation_model_key, "tts": tts_model_key},
                    }
                    return lang, payload
    
                results = await asyncio.gather(*(process_language(lang) for lang in languages_to_process))
                for lang, payload in results:
                    language_payloads[lang] = payload
                    aligned = payload.get("subtitles", {}).get("aligned", {})
                    if aligned:
                        subtitles_per_language[lang] = {"aligned": aligned}
    
                primary_payload = None
                if default_language and default_language in language_payloads:
                    primary_payload = language_payloads[default_language]
                elif language_payloads:
                    default_language = next(iter(language_payloads))
                    primary_payload = language_payloads[default_language]
    
                if primary_payload:
                    final_video_path = primary_payload.get("final_video_path", final_video_path)
                    default_audio_path = primary_payload.get("final_audio_path", "")
                    default_speech_track = primary_payload.get("speech_track", "")
                    aligned = primary_payload.get("subtitles", {}).get("aligned", {})
                    srt_path_1 = aligned.get("srt", srt_path_1)
                    vtt_path_1 = aligned.get("vtt", vtt_path_1)
    
            def keep_if_persistent(path: Optional[str | Path]) -> str:
                if not path:
                    return ""
                return str(path) if (workspace.persist_intermediate or not source_has_video) else ""
    
            default_audio_path = keep_if_persistent(default_audio_path)
            default_speech_track = keep_if_persistent(default_speech_track)
    
            language_outputs_serialized: Dict[str, Dict[str, Any]] = {}
            for lang, payload in language_payloads.items():
                aligned = payload.get("subtitles", {}).get("aligned", {})
                intermediate_serialized = {
                    key: keep_if_persistent(value)
                    for key, value in (payload.get("intermediate_files") or {}).items()
                }
                language_outputs_serialized[lang] = {
                    "final_video_path": payload.get("final_video_path", ""),
                    "final_audio_path": keep_if_persistent(payload.get("final_audio_path")),
                    "speech_track": keep_if_persistent(payload.get("speech_track")),
                    "subtitles": {
                        "aligned": {
                            "srt": keep_if_persistent(aligned.get("srt")),
                            "vtt": keep_if_persistent(aligned.get("vtt")),
                        }
                    },
                    "intermediate_files": intermediate_serialized,
                    "models": payload.get("models", per_language_models.get(lang, {})),
                }
    
            for lang, subs in subtitles_per_language.items():
                aligned = subs.get("aligned", {})
                existing = language_outputs_serialized.setdefault(
                    lang,
                    {
                        "final_video_path": final_video_path if default_language == lang else "",
                        "final_audio_path": "",
                        "speech_track": "",
                        "subtitles": {},
                        "intermediate_files": {},
                        "models": per_language_models.get(lang, {}),
                    },
                )
                existing.setdefault("subtitles", {})
                existing["subtitles"]["aligned"] = {
                    "srt": keep_if_persistent(aligned.get("srt")),
                    "vtt": keep_if_persistent(aligned.get("vtt")),
                }
    
            subtitles_per_language_payload = {
                lang: data.get("subtitles", {})
                for lang, data in language_outputs_serialized.items()
                if data.get("subtitles")
            }
    
            if per_language_models:
                selected_models["per_language"] = per_language_models
    
            default_intermediate = (language_outputs_serialized.get(default_language) or {}).get("intermediate_files", {})
    
            intermediate_files_payload: Dict[str, Any] = {
                "asr_original": keep_if_persistent(asr_raw_path),
                "asr_aligned": keep_if_persistent(asr_aligned_path),
                "translation": default_intermediate.get("translation", ""),
                "translation_aligned_W_origin": default_intermediate.get("translation_aligned_W_origin", ""),
                "translation_aligned_W_dubbedvoice": default_intermediate.get("translation_aligned_W_dubbedvoice", ""),
                "tts": default_intermediate.get("tts", ""),
                "vocals": keep_if_persistent(vocals_path),
                "background": keep_if_persistent(background_path),
                "per_language": {
                    lang: data["intermediate_files"] for lang, data in language_outputs_serialized.items()
                },
            }
    
            subtitles_payload: Dict[str, Any] = {
                "original": {"srt": keep_if_persistent(srt_path_0), "vtt": keep_if_persistent(vtt_path_0)},
                "aligned": {"srt": keep_if_persistent(srt_path_1), "vtt": keep_if_persistent(vtt_path_1)},
            }
            if subtitles_per_language_payload:
                subtitles_payload["per_language"] = {
                    lang: {"aligned": subs.get("aligned", {})}
                    for lang, subs in subtitles_per_language_payload.items()
                    if subs.get("aligned")
                }
    
            final_result: Dict[str, Any] = {
                "workspace_id": workspace.workspace_id,
                "final_video_path": final_video_path,
                "final_audio_path": default_audio_path,
                "speech_track": default_speech_track,
                "source_media": original_source,
                "source_video": keep_if_persistent(source_media_local_path),
                "source_media_local_path": str(source_media_local_path) if source_media_local_path else "",
                "default_language": default_language,
                "available_languages": list(language_outputs_serialized.keys()),
                "language_outputs": language_outputs_serialized,
                "models": selected_models,
                "subtitles": subtitles_payload,
                "intermediate_files": intermediate_files_payload,
                "timings": step_timer.timings,
            }
    
            workspace.maybe_dump_json("final_result.json", final_result)
    
            # FIX: Copy to persistent storage if on Modal
            try:
                await copy_to_persistent_storage(workspace.workspace_id, workspace.workspace)
            except Exception as exc:
                logger.error(f"Failed to persist outputs: {exc}", exc_info=True)
    
            return final_result
    
        except asyncio.CancelledError:
            cancelled = True
            raise
        except HTTPException:
            raise
        except Exception as exc:  # noqa: BLE001
            logger.exception("Pipeline failed: %s", exc)
>           raise HTTPException(500, f"Pipeline failed: {exc}") from exc
E           fastapi.exceptions.HTTPException: 500: Pipeline failed: name 'tr_provider' is not defined

app/main.py:2499: HTTPException
------------------------------ Captured log call -------------------------------
ERROR    bluez.orchestrator:main.py:2498 Pipeline failed: name 'tr_provider' is not defined
Traceback (most recent call last):
  File "/home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator/app/main.py", line 2359, in dub
    results = await asyncio.gather(*(process_language(lang) for lang in languages_to_process))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator/app/main.py", line 2184, in process_language
    tr_provider,  # ADD: provider
    ^^^^^^^^^^^
NameError: name 'tr_provider' is not defined
=============================== warnings summary ===============================
<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

app/main.py:1263
  /home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator/app/main.py:1263: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("startup")

.venv/lib/python3.11/site-packages/fastapi/applications.py:4574
.venv/lib/python3.11/site-packages/fastapi/applications.py:4574
  /home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator/.venv/lib/python3.11/site-packages/fastapi/applications.py:4574: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    return self.router.on_event(event_type)

app/main.py:1273
  /home/runner/work/ProYouTubers-Dubbing-Studio/ProYouTubers-Dubbing-Studio/apps/backend/services/orchestrator/app/main.py:1273: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("shutdown")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_pipeline_integration.py::test_dub_pipeline_minimal - fastapi.exceptions.HTTPException: 500: Pipeline failed: name 'tr_provider' is not defined
FAILED tests/test_pipeline_integration.py::test_dub_pipeline_multiple_languages - fastapi.exceptions.HTTPException: 500: Pipeline failed: name 'tr_provider' is not defined
======================== 2 failed, 6 warnings in 13.78s ========================
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
make: *** [Makefile:71: test] Error 1
Error: Process completed with exit code 2.